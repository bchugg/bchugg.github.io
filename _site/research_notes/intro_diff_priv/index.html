<!DOCTYPE html>
<html lang="en-us">

  <head prefix="og: http://ogp.me/ns#; dc: http://purl.org/dc/terms/#">
  
  
  <!-- Canonical link to help search engines -->
  <link rel="canonical" href="http://localhost:4000/research_notes/intro_diff_priv/" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PN2H9928PZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PN2H9928PZ');
  </script>

  <!-- Basic meta elements -->
  <meta charset="utf-8" />

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width,initial-scale=1.0,shrink-to-fit=no" />

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <title>
    
     Introduction to Differential Privacy
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="Introduction to Differential Privacy" />
<meta name="author" content="Ben Chugg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The Laplace mechanism, composition theorems, and local privacy" />
<meta property="og:description" content="The Laplace mechanism, composition theorems, and local privacy" />
<link rel="canonical" href="http://localhost:4000/research_notes/intro_diff_priv/" />
<meta property="og:url" content="http://localhost:4000/research_notes/intro_diff_priv/" />
<meta property="og:site_name" content="benny" />
<meta property="og:image" content="http://localhost:4000/assets/writing_images/cybersecurity.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-24T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:image" content="http://localhost:4000/assets/writing_images/cybersecurity.jpg" />
<meta property="twitter:title" content="Introduction to Differential Privacy" />
<meta name="twitter:site" content="@bennychugg" />
<meta name="twitter:creator" content="@Ben Chugg" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Chugg","url":"https://benchugg.com"},"dateModified":"2022-12-24T00:00:00-08:00","datePublished":"2022-12-24T00:00:00-08:00","description":"The Laplace mechanism, composition theorems, and local privacy","headline":"Introduction to Differential Privacy","image":"http://localhost:4000/assets/writing_images/cybersecurity.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research_notes/intro_diff_priv/"},"url":"http://localhost:4000/research_notes/intro_diff_priv/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Dublin Core metadata for Zotero -->
  <meta property="dc:title" content="Introduction to Differential Privacy" />
  <meta property="dc:creator" content="Ben Chugg" />
  <meta property="dc:identifier" content="http://localhost:4000/research_notes/intro_diff_priv/" />
  
  <meta property="dc:date" content="2022-12-24 00:00:00 -0800" />
  
  <meta property="dc:source" content="benny" />

  <!-- Open Graph and Twitter metadata -->
  <meta property="og:title" content="Introduction to Differential Privacy" />
  <meta property="og:url" content="http://localhost:4000/research_notes/intro_diff_priv/" />
  
    <meta property="og:image" content="http://localhost:4000/assets/writing_images/cybersecurity.jpg" />
    <meta name="twitter:image" content="http://localhost:4000/assets/writing_images/cybersecurity.jpg" />
  
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:card" content="summary">
  <meta name="twitter:domain" value="benchugg.com" />
  <meta name="twitter:title" value="Introduction to Differential Privacy" />
  <meta name="twitter:url" value="http://localhost:4000" />
  
  <!-- Description is dependent on page type  -->
  
    <meta property="og:description" content="The Laplace mechanism, composition theorems, and local privacy">
    <meta name="twitter:description" value="The Laplace mechanism, composition theorems, and local privacy" />
    <meta property="og:type" content="article" />
  
  

  <!-- CSS link -->
  <link rel="stylesheet" href="/assets/css/style.css" />

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="167x167" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="180x180" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon.ico">
  <link rel="shortcut icon" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="114×114" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="72×72" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” href="/assets/favicon.ico" />
  <link rel=”icon” href="/assets/favicon.ico", sizes="32x32"/>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml" />

  
</head>


  <body class="theme-base-">


    <!--
      Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS
      collisions with our real content.
    -->
    <div class="wrap">
      <header class="masthead">
        <div class="container">
          <a id='name' href="/">Ben Chugg</a>
          <div class="span">
            <a href="/papers/">Papers</a> 
            <a href="/writing/">Writing</a>
            <a href="/research_notes/">Notes</a>
            <a href="http://incrementspodcast.com">Podcast</a>
          </div>
        </div>
      </header>

      <main class="container content" id="main">
        <article class="note">
  <small>
    <a id="back", href="/research_notes/">Back to all notes</a>
  </small>
  <hr>
  
  <p class='title'>Introduction to Differential Privacy</p> 
  <p id="date">December 24, 2022</p>
  \[\renewcommand{\lap}{\text{Lap}}
\newcommand{\eps}{\epsilon}
\newcommand{\range}{\text{range}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\image}{\text{image}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\D}{\mathcal{D}}\]

<ul id="markdown-toc">
  <li><a href="#1-example-the-laplace-mechanism" id="markdown-toc-1-example-the-laplace-mechanism">1. Example: The Laplace Mechanism</a></li>
  <li><a href="#2-composition" id="markdown-toc-2-composition">2. Composition</a></li>
  <li><a href="#3-local-differential-privacy" id="markdown-toc-3-local-differential-privacy">3. Local differential Privacy</a>    <ul>
      <li><a href="#31-warners-randomized-response" id="markdown-toc-31-warners-randomized-response">3.1 Warner’s randomized response</a></li>
    </ul>
  </li>
</ul>

<p>I remember attending some reading groups on differential privacy during my undergraduate days. My main takeaway was “add noise to data and privacy increases.” That seemed rather obvious, so I stopped attending and gradually lost interest in the subject.  It didn’t help that, at that point, differential privacy wasn’t being relied on in practice.</p>

<p>After few years I can safely say that I regret my choice. Differential privacy is now used in a lot of applications. Apple <a rel="nofollow noopener noreferrer" href="https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf">started using it in macOS Sierra</a> and has since expanded its application to Safari. Google <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2107.01179.pdf">used differential privacy</a> when gathering insights from searches related to Covid-19, and for their <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2004.04145.pdf">Covid-19 mobility reports</a>. Other examples include <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2010.13981.pdf">LinkedIn</a>, <a rel="nofollow noopener noreferrer" href="https://proceedings.neurips.cc/paper/2017/file/253614bbac999b38b5b60cae531c4969-Paper.pdf">Microsoft</a> and <a rel="nofollow noopener noreferrer" href="https://www.usenix.org/conference/enigma2018/presentation/ensign">Uber</a>. What didn’t occur to me was that that <em>how</em> you add noise matters, and developing methods to add the right amount of noise in the right ways is an interesting problem.</p>

<p>Intuitively, a differentially private mechanism should be one which is not excessively reactive to small changes in the dataset. Let \(\D\) be the set of all databases, and consider a function \(g\) which acts on \(\D\). (Think of a database as just some big data table where, e.g., each row corresponds to a different user.) For instance, given a database \(x\in \D\), \(g(x)\) might be the size of \(x\), or the total amount user deposits in \(x\), or it might ask for the number of users with some property (a “counting query”).</p>

<p>We want to add noise to \(g\) in such a way that, regardless of how it queries the database, it cannot back out sensitive information. More precisely, we want to ensure that if there are two databases which are nearly identitical, then asking the same question of each will not betray anyone’s private data. For example, if \(g(x)\) is the total assets across all users in the database \(x\), and \(z\) equals \(x\) except that it omits a single row, then \(g(x)-g(z)\) tells you about the assets of the omitted user.  Similar examples abound: medical diagnoses, debts, etc.</p>

<p>Formally, we call a (randomized) mechanism \(f\) which acts on databases \((\eps,\delta)\)-differentially private if, for all outputs \(A\subset \text{Range}(f)\) and all databases \(x\) and \(z\) such that \(x\) and \(z\) differ by one row,</p>

\[\Pr(f(x)\in A)\leq e^\eps \Pr(f(z)\in A) + \delta.\]

<p>If \(\delta=0\), we call \(f\) \(\eps\)-differentially private.</p>

<p>The intuition behind the definition is easier to grasp if we negate it. If \(f\) is <em>not</em> differentially private, this means there exists some \(A\) such that \(\Pr(f(x)\in A)\gg \Pr(f(z) \in A)\). That is, swapping just a single row of \(x\) to \(z\) changed the output distribution considerably, meaning that \(f\) is sensitive to the data. If \(f\) is differentiably private then, roughly speaking, small changes in the input result in small changes in the output.</p>

<p>A popular to write \(\eps\)-differential privacy is as the likelihood ratio</p>

\[\sup_{A\in \image(f)} \sup_{x_1,x_2:x_1\in \delta_1(x_2)}\frac{\Pr(f(x_1)\in A)}{\Pr(f(x_2)\in A)}\leq e^\eps,\]

<p>where \(\delta_1(x)\) is the set of databases which different by at most 1 row from \(x\).</p>

<h1 id="1-example-the-laplace-mechanism">1. Example: The Laplace Mechanism</h1>

<p>Consider a deterministic function \(g:\D\to \R^k\).  For instance, \(g\) might ask how many users obey \(k\) properties. 
Define</p>

\[\Delta = \sup_{x,y:x\in \delta_1(x)} ||g(x) - g(y)||_1 = \sup_{x,y:x\in\delta_1(y)} \sum_{i=1}^k |g(x)_i - g(y)_i|.\]

<p>\(\Delta\) is often called the \(\ell_1\)-<em>sensitivity</em> of \(g\). The Laplace mechanism 
is defined as</p>

\[f(x) = g(x) + (Y_1,\dots,Y_k),\]

<p>where \(Y_1,\dots,,Y_k\sim\lap(0,\Delta/\eps)\) independently. Recall that the Laplacian distribution \(\lap(a,b)\) has pdf \(p(x) = (2b)^{-1}\exp(-\vert x-a\vert /b)\). To show that this mechanism is differentially private, we show that</p>

\[\int_A \Pr(f(x)=z)dz \leq e^\eps \int_A \Pr(f(y)=z)dz,\]

<p>for all \(x\in\delta_1(y)\). To see this, it suffices to show that for all \(z\in A\), \(\Pr(f(z)=z)\leq e^\eps \Pr(f(y)=z\)) and then to integrate over \(A\). Note that,</p>

\[\begin{align}
\Pr(f(x)=z) &amp;= \prod_{i=1}^k \Pr(M(x)_i = z_i) = \prod_{i=1}^k \Pr(Y_i = z_i - g(x)_i) \\
&amp;= \prod_{i=1}^k \frac{\eps}{\Delta} \exp(-\frac{\eps}{\Delta}|z_i - g(x)_i|).
\end{align}\]

<p>Therefore, by the reverse triangle inequality,</p>

\[\begin{align}
\frac{\Pr(f(x)=z)}{\Pr(f(y)=z)} &amp;= \prod_{i=1}^k \exp\bigg(\frac{\eps}{\Delta}(|z_i - g(y)_i| - |z_i - g(x)_i|)\bigg) \\ 
&amp;\leq \prod_{i=1}^k \exp\bigg(\frac{\eps}{\Delta}(|g(y)_i- g(x)_i|)\bigg) \\ 
&amp;= \exp\bigg(\frac{\eps}{\Delta}\sum_{i=1}^k |g(y)_i - g(x)_i|\bigg) \leq \exp(\eps),
\end{align}\]

<p>which shows that the Laplace mechanism is \((\eps,0)\)-differentially private.</p>

<h1 id="2-composition">2. Composition</h1>

<p>One immediate question is how differentially private mechanisms behave under composition. For instance, can we employ multiple differentially private algorithms in tandem and retain differential privacy? Sort of.</p>

<p>Suppose \(f_1,f_2\) are \((\eps_1,\delta_1)\), \((\eps_2,\delta_2)\) differentially private, respectively. Considering concatenating the output so that, given a database \(x\), we construct a new map \(g\) such that \(g(x) = (f_1(x), f_2(x))\), where we run \(f_1\) and \(f_2\) independently of one another. Will \(g\) be differentially private?</p>

<p>It turns out that \(g\) will be \((\eps_1+\eps_2,\delta_1+\delta_2)\) differentially private. You’ll often see this result referred to as “the epsilons and deltas add up.” Unfortunately, when people cite this result, they typically cite Theorem 1 of <a rel="nofollow noopener noreferrer" href="https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf">this paper</a>, which has an incorrect proof.  There is a corrected proof in the <a rel="nofollow noopener noreferrer" href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">book by Dwork and Roth</a>, but I think it’s more complicated than necessary. Here’s a simpler proof.</p>

<p>Note the range of \(g\) is the product space \(\range(f_1)\times\range(f_2)\).  Since we run \(f_1\) and \(f_2\) independently, we have, for any \(A\times B\in range(g)\) and any neighboring databases \(x\) and \(z\),</p>

\[\begin{align*}\Pr(g(x)\in A\times B)&amp;=\Pr((f_1(x),f_2(x))\in A\times B) \\ &amp;= \Pr(f_1(x)\in A)\Pr(f_2(x)\in B)  \\ 
&amp;\leq \min\{e^{\eps_1}\Pr(f_1(z)\in A+\delta_1,1\}\Pr(f_2(x)\in B) \\ 
&amp;\leq (\min\{e^{\eps_1}\Pr(f_1(z)\in A),1\} + \delta_1)\Pr(f_2(x)\in B) \\ 
&amp;\leq \min\{e^{\eps_1}\Pr(f_1(z)\in A),1\}\Pr(f_2(x)\in B) + \delta_1 \\ 
&amp;\leq \min\{e^{\eps_1}\Pr(f_1(z)\in A),1\}(e^{\eps_2}\Pr(f_2(z)\in B) + \delta_2) + \delta_1 \\ 
&amp;\leq \min\{e^{\eps_1}\Pr(f_1(z)\in A),1\}(e^{\eps_2}\Pr(f_2(z)\in B)  + \delta_2 + \delta_1 \\ 
&amp;\leq e^{\eps_1+\eps_2} \Pr(f_1(z) \in A)\Pr(f_2(z)\in B) + \delta_1+\delta_2 \\ 
&amp;= e^{\eps_1+\eps_2} \Pr(g(z) \in A\times B) + \delta_1+\delta_2.
\end{align*}\]

<p>Here we’ve just used basic facts of the min function and recognized that probabilities can be at most 1. Of course, we can extend this result by induction and conclude that given finitely many mechanisms \(\{f_i\}\) where \(f_i\) is \((\eps_i,\delta_i)\)-differentially private, then the mechanism \(g:\X\to \bigotimes_i \range(f_i)\) given by  \(g:x\mapsto \otimes_i f_i(x)\) is \((\sum_i \eps_i,\sum_i\delta_i)\) differentially private.</p>

<p>We can also consider deterministic composition, and demonstrate that it does not affect the privacy guarantees. Indeed, suppose \(f:\D\to U\) is \((\eps,\delta)\) private and let \(g:U\to V\) be deterministic and invertible. Then for any \(W\subset V\),</p>

\[\begin{align}
\Pr(f\circ g(x) \in W) &amp;= \Pr(f(x) \in g^{-1}(W)) \\
&amp;\leq e^\eps \Pr(f(y) \in g^{-1}(W)) = \Pr(f\circ g(y)\in W),
\end{align}\]

<p>so \(f\circ g\) is  \((\eps,\delta)\) private.</p>

<h1 id="3-local-differential-privacy">3. Local differential Privacy</h1>

<p>So far the model we’ve posited is one where there exists some large database \(x\in \X\) which contains all users’ data, and then some administrator or custodian of the data privatizes it. In practice, this isn’t terribly secure. Do we trust the custodian? Does she delete the original database after it’s privatized? What if another company offers her a lot of money for \(x\), or her company is bought by another?</p>

<p>For all these reasons, we might consider <em>local</em> differentially privacy. Here, the data is privatized on the users end before reaching any centralized database. Thus, no one sees the raw data besides the user themselves. Google <a rel="nofollow noopener noreferrer" href="https://ai.google/research/pubs/pub42852">uses</a> local differential privacy to collect information from users’ browsers, and Apple <a rel="nofollow noopener noreferrer" href="https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html">uses</a> local differential privacy to collect emoji data.</p>

<p>In the local setting, instead of considering functions which act on the set of databases \(\D\), we consider functions which act on a users private data. If \(h\) is such a function, then we say that \(h\) is \(\eps\)-local differentially private if, for all user data \(x\) and \(y\) and all \(A\subset \image(h)\),</p>

\[\Pr(h(x)\in A)\leq e^\eps \Pr(h(y)\in A).\]

<p>Mathematically, therefore, this looks precisely the same as the definition of \((\eps,0)\)-differential privacy, except that the function \(h\) is acting on a different space than the function \(f\).</p>

<h2 id="31-warners-randomized-response">3.1 Warner’s randomized response</h2>

<p>An example of an algorithm which is locally differentially private is Warner’s method for survey responses. The idea is to enable respondents to answer potentially sensitive survey questions while maintaining plausible deniability.</p>

<p>Consider a survey with a sensitive yes/no question and fix some \(r\in(0,1]\). The respondent answers truthfully with probability \(r\), otherwise flips an unbiased coin. That is, the privatized response is</p>

\[Z\sim \begin{cases} X,&amp; \text{w.p. } r\\ Y\sim \text{Bernoulli(1/2)},&amp;\text{w.p }1-r
\end{cases}\]

<p>Note that \(\Pr(Z=z\vert X=x) = r\ind(z=x) + (1-r)/2\). Therefore,</p>

\[\max_z\max_x \frac{\Pr(Z|X=x)}{\Pr(Z|X=x')} = \max_x\max_z \frac{r\ind(z=x) + (1-r)/2}{r\ind(z=x') + (1-r)/2} = 1 + \frac{2r}{1-r}.\]

<p>If we set \(\eps=\log(1 + 2r/(1-r))\), we see that Warner’s randomized response is \(\eps\)-locally differentiably private.</p>


  <small>
    <a link="back", href="/research_notes/">Back to all notes</a>
  </small>
</article>
 <!-- "> -->
      </main>

      <!-- Footer  -->
      <div class="container" id="footer">
        <hr>
        <div class='container links'>
          <a href="https://github.com/bchugg" rel='nofollow'><img src="/assets/images/github.png"></a>
          <a href="https://www.goodreads.com/user/show/90945992-ben-chugg" rel='nofollow'><img src="/assets/images/goodreads.png"></a>
          <a href="https://twitter.com/BennyChugg" rel='nofollow'><img src="/assets/images/twitter.png"></a>
        </div>
      
        <div class='copyright'>
          <small>&#169; 2023 Ben Chugg</small>
        </div>
      
      </div>
    </div>

  </body>
</html> 
