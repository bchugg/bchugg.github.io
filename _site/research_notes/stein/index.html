<!DOCTYPE html>
<html lang="en-us">

  <head prefix="og: http://ogp.me/ns#; dc: http://purl.org/dc/terms/#">
  
  
  <!-- Canonical link to help search engines -->
  <link rel="canonical" href="http://localhost:4000/research_notes/stein/" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PN2H9928PZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PN2H9928PZ');
  </script>

  <!-- Basic meta elements -->
  <meta charset="utf-8" />

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width,initial-scale=1.0,shrink-to-fit=no" />

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <title>
    
     Degrees of Freedom and Stein's Estimator
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="Degrees of Freedom and Stein’s Estimator" />
<meta name="author" content="Ben Chugg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Minimum risk estimators for normal random vectors: Stein’s paradox" />
<meta property="og:description" content="Minimum risk estimators for normal random vectors: Stein’s paradox" />
<link rel="canonical" href="http://localhost:4000/research_notes/stein/" />
<meta property="og:url" content="http://localhost:4000/research_notes/stein/" />
<meta property="og:site_name" content="benny" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-05T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Degrees of Freedom and Stein’s Estimator" />
<meta name="twitter:site" content="@bennychugg" />
<meta name="twitter:creator" content="@Ben Chugg" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Chugg","url":"https://benchugg.com"},"dateModified":"2022-07-05T00:00:00-07:00","datePublished":"2022-07-05T00:00:00-07:00","description":"Minimum risk estimators for normal random vectors: Stein’s paradox","headline":"Degrees of Freedom and Stein’s Estimator","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research_notes/stein/"},"url":"http://localhost:4000/research_notes/stein/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Dublin Core metadata for Zotero -->
  <meta property="dc:title" content="Degrees of Freedom and Stein's Estimator" />
  <meta property="dc:creator" content="Ben Chugg" />
  <meta property="dc:identifier" content="http://localhost:4000/research_notes/stein/" />
  
  <meta property="dc:date" content="2022-07-05 00:00:00 -0700" />
  
  <meta property="dc:source" content="benny" />

  <!-- Open Graph and Twitter metadata -->
  <meta property="og:title" content="Degrees of Freedom and Stein's Estimator" />
  <meta property="og:url" content="http://localhost:4000/research_notes/stein/" />
  
    <meta property="og:image" content="http://localhost:4000/assets/images/heads.jpeg"/>
    <meta name="twitter:image" content="http://localhost:4000/assets/images/heads.jpeg" />
  
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:card" content="summary">
  <meta name="twitter:domain" value="benchugg.com" />
  <meta name="twitter:title" value="Degrees of Freedom and Stein's Estimator" />
  <meta name="twitter:url" value="http://localhost:4000" />
  
  <!-- Description is dependent on page type  -->
  
    <meta property="og:description" content="Minimum risk estimators for normal random vectors: Stein's paradox ">
    <meta name="twitter:description" value="Minimum risk estimators for normal random vectors: Stein's paradox " />
    <meta property="og:type" content="article" />
  
  

  <!-- CSS link -->
  <link rel="stylesheet" href="/assets/css/style.css" />

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="167x167" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="180x180" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon.ico">
  <link rel="shortcut icon" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="114×114" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="72×72" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” href="/assets/favicon.ico" />
  <link rel=”icon” href="/assets/favicon.ico", sizes="32x32"/>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml" />

  
</head>


  <body class="theme-base-">


    <!--
      Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS
      collisions with our real content.
    -->
    <div class="wrap">
      <header class="masthead">
        <div class="container">
          <a id='name' href="/">Ben Chugg</a>
          <div class="span">
            <a href="/papers/">Papers</a> 
            <a href="/writing/">Writing</a>
            <a href="/research_notes/">Notes</a>
            <a href="http://incrementspodcast.com">Podcast</a>
          </div>
        </div>
      </header>

      <main class="container content" id="main">
        <article class="note">
  <small>
    <a id="back", href="/research_notes/">Back to all notes</a>
  </small>
  <hr>
  
  <p class='title'>Degrees of Freedom and Stein's Estimator</p> 
  <p id="date">July 05, 2022</p>
  \[\newcommand{\R}{\mathbb{R}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\norm}[1]{\vert\vert #1 \vert\vert}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\df}{\text{df}}
\newcommand{\bZ}{\bar{Z}}\]

<ul id="markdown-toc">
  <li><a href="#1-steins-lemma" id="markdown-toc-1-steins-lemma">1. Stein’s Lemma</a></li>
  <li><a href="#2-steins-paradox" id="markdown-toc-2-steins-paradox">2. Stein’s Paradox</a>    <ul>
      <li><a href="#21-optimizing-v" id="markdown-toc-21-optimizing-v">2.1 Optimizing \(v\)</a></li>
      <li><a href="#22-multiple-samples" id="markdown-toc-22-multiple-samples">2.2 Multiple Samples</a></li>
      <li><a href="#23-discussion" id="markdown-toc-23-discussion">2.3 Discussion</a></li>
    </ul>
  </li>
  <li><a href="#3-resources" id="markdown-toc-3-resources">3. Resources</a></li>
</ul>

<p>Given random vectors \(X_i \sim N(\mu,\sigma^2I)\), \(i=1,\dots,n\), for \(\sigma\in\R\) and \(\mu\in\R^d\), we want to generate and evaluate an estimate \(\hmu(X)\) of \(\mu\).</p>

<p>The <em>risk</em> of the estimate \(\hmu(X)\) is defined as its expected MSE:</p>

\[R(\hmu(X)) \equiv \E\norm{\mu-\hmu(X)}_2^2.\]

<p>The expectation is over randomness in the \(X_i\) and any randomness in the estimator itself. The notion of risk enables us to develop hierarchies of estimators. We say the estimator \(\hmu_1(X)\) <em>dominates</em> \(\hmu_2(X)\) if \(R(\hmu_1(X))\leq R(\hmu_2(X))\) for all \(\mu\in\R^d\), and there exists at least one \(\mu\) such that \(R(\hmu_1(X)) &lt; R(\hmu_2(X))\), i.e., \(\mu_1\) is strictly better than \(\mu_2\). If an estimator \(\hmu\) is dominated by another, we say it is <em>inadmissible</em>.</p>

<p>We’ll often drop the dependence on \(X\) when it’s clear from context.</p>

<p>There’s a general technique to evaluate the risk of an estimator which involves expanding out its expression. Consider a single random vector \(y\sim N(\mu,\sigma^2I)\)</p>

\[\begin{align}
R(\hmu) &amp;= \E\norm{\mu-y+y-\hmu}_2^2 \\
&amp;= \E\norm{\mu-y}_2^2 + \E\norm{y-\hmu}_2^2 + 2\E(\mu-y)^t(y-\hmu).
\end{align}\]

<p>The first term is</p>

\[\frac{1}{\sigma\sqrt{2\pi}}\int_{\R^n}(\mu-y)^t(\mu-y)\exp\bigg(-\frac{1}{2}\bigg(\frac{y-\mu}{\sigma}\bigg)^2\bigg)dy,\]

<p>which is an \(n\)-dimensional second moment and equal to \(n\sigma^2\) using some <a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Normal_distribution#Moments">standard identities</a>. The final term can be expanded as</p>

\[\begin{align}
\E(\mu-y)^t(y-\hmu) &amp;= \sum_{i=1}^n \E[\mu_iy_i - \mu_i\hmu_i -y_i^2 + y_i\hmu_i] \\ 
&amp;= \sum_{i=1}^n (\mu_i^2 - \mu_i\E[\hmu_i] -\E[y_i^2] + \E[y_i\hmu_i]) \\ 
&amp;= \sum_{i=1}^n (\Cov(y_i,\hmu_i)-\Var(y_i)) \\
&amp;= \sum_{i=1}^n \Cov(y_i,\hmu_i) - n\sigma^2. 
\end{align}\]

<p>Therefore,</p>

\[R(\hmu) = \E\norm{y-\hmu}_2^2 + 2\sum_{i=1}^n \Cov(y_i,\hmu_i) - n\sigma^2.\]

<p>If we view the estimator \(\hmu\) as a predictive model trained on the samples \(X_i\), then we can interpret \(\E\norm{y-\hmu}_2^2\) as the expected mean squared (training) error of the model. The <em>degrees of freedom</em> (see, e.g., <a rel="nofollow noopener noreferrer" href="https://www.stat.cmu.edu/~ryantibs/papers/sdf.pdf">this paper</a>) of this model is the quantity</p>

\[\df(\hmu) = \frac{1}{\sigma^2}\sum_{i=1}^n \Cov(y_i,\hmu_i).\]

<p>Note that this definition accords fairly well with the standard notion of degrees of freedom, which is roughly the number of parameters depended on by the model \(\hmu\). If \(\hmu\) does not use the \(i\)-th component of \(y\) to form its estimate, the covariance will be zero and the degrees of freedom will decrease.</p>

<p>Putting this all together, we can write the risk as</p>

\[\begin{equation}
\label{eq:risk}
R(\hmu) = \E\norm{y-\hmu}_2^2 + 2\sigma^2 \df(\hmu) - n\sigma^2.\tag{1}
\end{equation}\]

<h1 id="1-steins-lemma">1. Stein’s Lemma</h1>

<p><a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> provides a nice way to evaluate the degrees of freedom of an estimator. It says that instead of evaluating the covariance, we can actually just take partial derivates of our estimator. That is,</p>

\[\frac{1}{\sigma^2}\sum_{i=1}^n \Cov(y_i,\hmu_i) = \sum_{i=1}^n\E\frac{\partial \hmu_i}{\partial x_i}(X).\]

<p>This is rather remarkable, because the right hand side has nothing to do with the actual values we observe from the distribution. We can therefore usually evaluate it much more easily.</p>

<p>Stated more generally, Stein’s lemma says that for a sufficiently smooth function \(f:\R^n\to\R\), and \(X\sim N(\mu,\sigma^2I)\), then</p>

\[\frac{1}{\sigma^2}\Cov(X,f(X)) = \frac{1}{\sigma^2}\E[(X-\mu)f(X)] = \E[\nabla f(X)].\]

<p>Stein’s lemma is the second equality; the first simply follows from the definition of covariance. The proof is relatively straightforward after getting past some administrative inconvenience.</p>

<p>First, we’ll assume that \(X\) is drawn from a standard normal (\(N(0,1)\)). If not, then we can perform the following proof on a rescaled version of \(X\) (and \(f\)) which will yield the same result. Next, let \(g(x)=\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^2}{2})\) be the density of the standard normal. Note that \(g'(x) = -xg(x)\). Finally, fix \(X\) and put \(f_i(x) = f(x,X_{-i})\), i.e., the behavior of \(f\) as a function of the \(i\)-th entry only. Here \(X_{-i} = (X_1,\dots,X_{i-1},X_{i+1},\dots,X_n)\). We’re going to show that</p>

\[\begin{equation}
\label{eq:stein}
\E[X_if_i(X_i)|X_{-i}] = \E[\frac{\partial f(X)}{\partial x_i}|X_{-i}], \tag{2}
\end{equation}\]

<p>from which the final result will follow by taking expectations over \(X_{-i}\).</p>

<p>Since \(X_i\) and \(X_{-i}\) are independent, the expectation on the right hand side of Equation \eqref{eq:stein} is one-dimensional, and we can use the fundamental theorem of calculus and Fubini’s theorem to write</p>

\[\begin{align}
\E[\frac{\partial f_i(X_i)}{\partial x_i}|X_{-i}] &amp;= \int_{-\infty}^\infty \partial f_i(x)g(x)dx \\ 
&amp;=  -\int_{-\infty}^\infty \partial f_i(x)\int_{-\infty}^x zg(z)dz dx \\
&amp;= \int_{0}^\infty \partial f_i(x)\int_{x}^\infty zg(z)dz dx -\int_{-\infty}^0 \partial f_i(x)\int_{-\infty}^x zg(z)dz dx \\
&amp;= \int_0^\infty zg(z)\int_0^z \partial f_i(x)dxdz - \int_{-\infty}^0 zg(z) \int_{z}^0 \partial f_i(x)dxdz \\ 
&amp;= \int_0^\infty zg(z)(f_i(z)-f_i(0))dz - \int_{-\infty}^0 zg(z)(f_i(0)- f_i(z)) dz \\
&amp;= \int_{-\infty}^\infty zg(z)f_i(z)dz = \E_z[zf_i(z)],
\end{align}\]

<p>as desired. The last line follows because \(\int_0^\infty zg(z)f_i(0)dz = -\int_{-\infty}^0 zg(z)f_i(0)dz\).</p>

<p>As a side note, I’ve seen it claimed that you can simply use integration by parts and write</p>

\[\int_{-\infty}^\infty \partial f_i(x)g(x)dx = f_i(x)g(x)\bigg\vert_{-\infty}^\infty - \int_{-\infty}^\infty f_i(x)g'(x)dx.\]

<p>Since, \(g'(x)=-xg(x)\), this would give the result provided that \(f_i(x)g(x)\bigg\vert_{-\infty}^\infty\) disappears. Unfortunately, I can’t see a way to do this without imposing additional assumptions on \(f\). While \(g(x)\to0\) as \(x\) goes to \(\infty\) or \(-\infty\), I don’t see why \(f_i(x)\) couldn’t grow quickly enough to offset this. All it would take is super-exponential growth at either end. If you figure this out, please tell me. In the meantime, Fubini it is.</p>

<p>Anyway, we can now write the risk of an estimator \(\hmu\) as</p>

\[\begin{equation}
\label{eq:stein_risk}
R(\hmu) = \E\norm{y-\hmu}_2^2 + 2\sigma^2 \sum_{i=1}^n \E\frac{\partial \hmu_i(X)}{\partial x_i} - n\sigma^2.\tag{3}
\end{equation}\]

<h1 id="2-steins-paradox">2. Stein’s Paradox</h1>

<p>Consider a slight variant of the problem we started with. Now you’re given random <em>variables</em> (i.e., not scalars, not vectors) \(X_1,\dots,X_n\) where \(X_i\sim N(\mu_i,\sigma^2)\). From this small amount of data (only one observation per distribution!), our task is to estimate \(\mu=(\mu_1,\dots,\mu_n)\).</p>

<p>An obvious estimate is to take \(\hmu(X)=X\). In this case, \(\partial \hmu_i/\partial X_i=1\), so Stein’s lemma tells us that</p>

\[R(\hmu(X)) = n\sigma^2.\]

<p>Remarkably, and counterintuitively (for me, at least), this is not the minimum risk estimator (i.e., it’s not admissible). Consider the <a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator">James-Stein estimator</a></p>

\[\hmu^{JS}(X,v) = \bigg(1 - \frac{n-2}{\norm{X-v}_2^2}\bigg)(X-v)+v,\]

<p>for any \(v\in \R^n\). Intuitively, this takes \(X\) and shrinks all components towards the vector \(v\). This is easy to see when \(v=0\), which is a helpful case to consider as an intuition pump. The \(i\)-th partial derivative of the James-Stein estimator is</p>

\[\frac{\partial \hmu_i^{JS}(X,v)}{\partial x_i} = 1 - (n-2)\bigg(\frac{1}{\norm{X-v}_2^2} - 2\frac{(X_i-v_i)^2}{\norm{X-v}_4^2}\bigg),\]

<p>so</p>

\[\sum_i\frac{\partial \hmu_i^{JS}(X,v)}{\partial x_i} = n  - \frac{n(n-2)}{\norm{X-v}_2^2} - \frac{2(n-2)}{\norm{X-v}_2^2}=n-\frac{(n-2)^2}{\norm{X-v}_2^2}.\]

<p>The expected training error is</p>

\[\begin{align}
\E\norm{X-\hmu^{JS}(X,v)}_2^2 &amp;= \E\sum_i\bigg(\bigg(\frac{n-2}{\norm{X-v}_2^2}\bigg)(X_i-v_i)\bigg)^2 \\
&amp;= \E\bigg[\frac{(n-2)^2}{\norm{X-v}_2^2}\sum_i (X_i-v_i)^2 \bigg]\\ 
&amp;= \E\bigg[\frac{(n-2)^2}{\norm{X-v}_2^2}\bigg]. 
\end{align}\]

<p>Combining all of this with Equation \eqref{eq:stein_risk} gives</p>

\[\begin{equation}
\label{eq:risk_js}
R(\hmu_v^{JS}(X,v)) = n\sigma^2 +(n-2)^2(1-2\sigma^2)\E\bigg[\frac{1}{\norm{X-v}_2^2}\bigg].\tag{4}
\end{equation}\]

<p>We’re interested in when this estimator performs better than the baseline estimate \(\hmu=X\), which has risk \(n\sigma^2\). Thus, \(R(\hmu_v^{JS}(X,v))&lt;R(X)\) iff</p>

\[(n-2)^2(1-2\sigma^2)\E\bigg[\frac{1}{\norm{X-v}_2^2}\bigg]&lt;0\]

<p>which occurs iff \(1-2\sigma^2&lt;0\), i.e., \(\sigma &gt; 1/\sqrt{2}\).</p>

<p>Unfortunately, even though the JS estimator demonstrates that the estimate \(\hmu=X\) is inadmissible, it itself is also inadmissible. This is because \(\hmu^{JS}\) is dominated by \(\hmu^{JS}_{+} \equiv \max\{\hmu^{JS},0\}\), where the maximum is taken point-wise. And, according to some classical results in point estimation, the minimum risk estimator must be smooth, so \(\hmu^{JS}_{+}\) is also inadmissible. The search continues!</p>

<p>A note on variance: While we assumed all normal distributions have the same variance, we don’t actually need to know what that variance is to form the JS estimate. Some resources seem to suggest that you do (e.g., Wikipedia), writing the JS estimator as</p>

\[\hmu^{JS}(X,v) = \bigg(1 - \frac{\sigma^2(n-2)}{\norm{X-v}_2^2}\bigg)(X-v)+v.\]

<p>This doesn’t seem to get you anything, however. The result is still the same: you need \(\sigma^2&gt;1/2\) for it to beat the canonical estimator.</p>

<p>It’s worth noting that you can prove this result without resorting to Stein’s lemma and degrees of freedom (e.g., see <a rel="nofollow noopener noreferrer" href="http://www.statslab.cam.ac.uk/~rjs57/SteinParadox.pdf">here</a>), but I think this way is more insightful and general.</p>

<h2 id="21-optimizing-v">2.1 Optimizing \(v\)</h2>

<p>It’s tempting to try and optimize the risk as a function of \(v\). However, this is challenging because the expectation in Equation \eqref{eq:risk_js} can’t be computed since we don’t know the distribution. If we receives multiple samples, we could use the empirical average instead, but in that case we’d be implicitly conditioning \(v\) on \(X\), in which case Equation \eqref{eq:risk_js} no longer holds. Indeed, if \(v\) is chosen as a function of \(X\), then the partial derivatives of the JS estimator  have extra terms:</p>

\[\sum_i\frac{\partial \hmu_i^{JS}(X,v)}{\partial x_i} = n - \frac{n-2}{\norm{X-v}_2^2}\bigg(\sum_i(1-\partial_i v_i) + 2\sum_i\frac{(X_i-v_i)^2(1-\partial_iv_i)}{\norm{X-v}_2^2}\bigg).\]

<p>That is, if \(\partial_iv_i\) is not zero, then the optimization problem becomes trickier.</p>

<h2 id="22-multiple-samples">2.2 Multiple Samples</h2>

<p>An obvious question is whether the result can be extended when multiples observations are drawn from \(N(\mu,\sigma^2I)\). The answer is yes. In that case, we consider the estimator</p>

\[\hmu_v = \bigg(1 - \frac{(n-2)\sigma^2}{k\norm{\bZ-v}_2^2}\bigg)(\bZ-v) + v,\]

<p>where</p>

\[\bZ = \frac{1}{k}\sum_i Z_i,\quad Z_i\sim N(\mu,\sigma^2I).\]

<p>The math is actually almost identical since the derivative of the average is simply 1.</p>

<h2 id="23-discussion">2.3 Discussion</h2>

<p>Stepping away from the math for a moment – this is kind of a crazy result. While the distributions are all assumed to be normal with the same variance, they don’t actually have anything to do with one another. One could be the distribution of female height in India, the other could be the test scores of high-school students in Japan (I think both of those are approximately normally distributed). The James-Stein estimator would have us generate estimates for each of these <em>which depend on the data drawn from the other distribution</em>. Japanese test-scores informing the height of Indian women? What on earth is going on here?</p>

<p>A rough answer is that the estimator expertly trades off bias and variance. By introducing some bias by correlating the independent observations, it’s able to lower the variance even further. Since MSE is a combination of bias and variance, if we sacrifice just enough bias, better overall risk can be achieved.</p>

<h1 id="3-resources">3. Resources</h1>

<ul>
  <li><a rel="nofollow noopener noreferrer" href="https://www.stat.cmu.edu/~larry/=sml/stein.pdf">Stein’s Unbiased Risk Estimate</a>, by Tibshirani and Wasserman (I basically stole their proof of Stein’s lemma verbatim since it’s so nice).</li>
</ul>


  <small>
    <a link="back", href="/research_notes/">Back to all notes</a>
  </small>
</article>
 <!-- "> -->
      </main>

      <!-- Footer  -->
      <div class="container" id="footer">
        <hr>
        <div class='container links'>
          <a href="https://github.com/bchugg" rel='nofollow'><img src="/assets/images/github.png"></a>
          <a href="https://www.goodreads.com/user/show/90945992-ben-chugg" rel='nofollow'><img src="/assets/images/goodreads.png"></a>
          <a href="https://twitter.com/BennyChugg" rel='nofollow'><img src="/assets/images/twitter.png"></a>
        </div>
      
        <div class='copyright'>
          <small>&#169; 2023 Ben Chugg</small>
        </div>
      
      </div>
    </div>

  </body>
</html> 
