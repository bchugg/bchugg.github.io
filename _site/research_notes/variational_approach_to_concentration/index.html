<!DOCTYPE html>
<html lang="en-us">

  <head prefix="og: http://ogp.me/ns#; dc: http://purl.org/dc/terms/#">
  
  
  <!-- Canonical link to help search engines -->
  <link rel="canonical" href="http://localhost:4000/research_notes/variational_approach_to_concentration/" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PN2H9928PZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PN2H9928PZ');
  </script>

  <!-- Basic meta elements -->
  <meta charset="utf-8" />

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width,initial-scale=1.0,shrink-to-fit=no" />

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <title>
    
     The variational approach to concentration
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="The variational approach to concentration" />
<meta name="author" content="Ben Chugg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Concentration for multivariate processess based on a variational inequality" />
<meta property="og:description" content="Concentration for multivariate processess based on a variational inequality" />
<link rel="canonical" href="http://localhost:4000/research_notes/variational_approach_to_concentration/" />
<meta property="og:url" content="http://localhost:4000/research_notes/variational_approach_to_concentration/" />
<meta property="og:site_name" content="benny" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-10T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The variational approach to concentration" />
<meta name="twitter:site" content="@bennychugg" />
<meta name="twitter:creator" content="@Ben Chugg" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Chugg","url":"https://benchugg.com"},"dateModified":"2024-06-10T00:00:00-04:00","datePublished":"2024-06-10T00:00:00-04:00","description":"Concentration for multivariate processess based on a variational inequality","headline":"The variational approach to concentration","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research_notes/variational_approach_to_concentration/"},"url":"http://localhost:4000/research_notes/variational_approach_to_concentration/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Dublin Core metadata for Zotero -->
  <meta property="dc:title" content="The variational approach to concentration" />
  <meta property="dc:creator" content="Ben Chugg" />
  <meta property="dc:identifier" content="http://localhost:4000/research_notes/variational_approach_to_concentration/" />
  
  <meta property="dc:date" content="2024-06-10 00:00:00 -0400" />
  
  <meta property="dc:source" content="benny" />

  <!-- Open Graph and Twitter metadata -->
  <meta property="og:title" content="The variational approach to concentration" />
  <meta property="og:url" content="http://localhost:4000/research_notes/variational_approach_to_concentration/" />
  
    <meta property="og:image" content="http://localhost:4000/assets/images/heads.jpeg"/>
    <meta name="twitter:image" content="http://localhost:4000/assets/images/heads.jpeg" />
  
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:card" content="summary">
  <meta name="twitter:domain" value="benchugg.com" />
  <meta name="twitter:title" value="The variational approach to concentration" />
  <meta name="twitter:url" value="http://localhost:4000" />
  
  <!-- Description is dependent on page type  -->
  
    <meta property="og:description" content="Concentration for multivariate processess based on a variational inequality">
    <meta name="twitter:description" value="Concentration for multivariate processess based on a variational inequality" />
    <meta property="og:type" content="article" />
  
  

  <!-- CSS link -->
  <link rel="stylesheet" href="/assets/css/style.css" />

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="167x167" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="180x180" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon.ico">
  <link rel="shortcut icon" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="114×114" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="72×72" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” href="/assets/favicon.ico" />
  <link rel=”icon” href="/assets/favicon.ico", sizes="32x32"/>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml" />

  
</head>


  <body class="theme-base-">


    <!--
      Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS
      collisions with our real content.
    -->
    <div class="wrap">
      <header class="masthead">
        <div class="container">
          <a id='name' href="/">Ben Chugg</a>
          <div class="span">
            <a href="/papers/">Papers</a> 
            <a href="/writing/">Writing</a>
            <a href="/research_notes/">Notes</a>
          </div>
        </div>
      </header>

      <main class="container content" id="main">
        <article class="note">
  <!-- <small>
    <a id="back", href="/research_notes/">Back to all notes</a>
  </small> -->
  <!-- <hr> -->
  
  <p class='title'>The variational approach to concentration</p> 
  <p id="date">June 10, 2024</p>
  <br/>
  \[\newcommand{\norm}[1]{\|#1\|}
\newcommand{\Re}{\mathbb{R}}
\renewcommand{\Mspace}[1]{\mathcal{M}({#1})}
\newcommand{\E}{\mathbb{E}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\sd}{\mathbb{S}^{d-1}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\kl}{D_{\text{KL}}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\d}{\text{d}}\]

<p>Let \((S_n)\) be some stochastic process in, say, \(\Re^d\). For instance, \(S_n = \sum_{i=1}^n X_i\) for multivariate observations \(X_i\in\Re^d\). We are aiming to generate a high probability bound on</p>

\[\norm{S_n} = \sup_{v:\norm{v}=1} \la v,S_n\ra.\]

<p>There are several well-known ways to attempt this. Among them are covering, chaining, and Doob decomposition (done, e.g., <a href="/research_notes/dim_free_bernstein/">here</a>). Here we’ll explore a separate (and relatively new) approach.</p>

<p>The idea is to use the variational inequality which is at the core of the <a href="/research_notes/pac_bayes/">PAC-Bayesian</a> methodology (so we could equivalently call this the PAC-Bayes approach to concentration).  This lets us simultaneously bound \(\la v,S_n\ra\) in each direction \(v\). Recall that a PAC-Bayes bound has the form</p>

\[\begin{equation}
\label{eq:pb-basic-bound}
\Pr( \forall \rho \in \Mspace{\Theta}: \text{Something holds}) \geq 1-\delta, \tag{1}
\end{equation}\]

<p>where \(\Theta\) is some parameter space and \(\Mspace{\Theta}\) is the set of all probability measures over that space. 
A PAC-Bayes bound provides a high probability bound <em>simultaneously</em> over all posteriors. The variational approach to concentration translates this into a high probability bound simultaneously over all directions.</p>

<p>It’s worth taking a moment to understand why this simultaneity property is valuable. A natural thought is to treat \(\la v,S_n\ra\) as a scalar-valued process and apply well understood concentration results for real-valued functions to it. Doing this in the naive way would give a separate bound for each \(v\in\sd\). So we’d have a result of the form:</p>

\[\forall v\in\sd, \Pr(\la v,S_n\ra \geq  B_n) \leq \delta.\]

<p>But now we’re stuck. This is not a bound on \(\sup_{v\in\sd}\la v,S_n\ra\). One’s first thought is to take a union bound over \(v\) in order to move the “\(\forall v\in\sd\)” inside the probability statement, which would give us the result. But there are uncountably many such vectors. The  approach explored here solves this problem by translating the “\(\forall \rho\in\Mspace{\Theta}\)” in \eqref{eq:pb-basic-bound} into “\(\forall v\in\sd\)”.</p>

<p>This variational approach was pioneered by Catoni and Giulini (<a rel="nofollow noopener noreferrer" href="https://ocatoni.perso.math.cnrs.fr/GramReg05.pdf">here</a> and <a rel="nofollow noopener noreferrer" href="https://ocatoni.perso.math.cnrs.fr/CatGiulNips03.pdf">here</a>), and has now been used by a few authors to prove bounds in a variety of settings:</p>
<ul>
  <li><a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2108.08198">Zhivotovskiy in 2024</a> for bounding the singular values of random matrices,</li>
  <li><a rel="nofollow noopener noreferrer" href="https://ui.adsabs.harvard.edu/abs/2022arXiv221009756N/abstract">Nakakita et al. in 2024</a> for bounding the mean of high-dimensional random matrices under heavy-tails,</li>
  <li><a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/1511.06259">Giulini in 2018</a> for estimating the Gram operator in Hilbert spaces,</li>
  <li><a rel="nofollow noopener noreferrer" href="https://arxiv.org/abs/2311.08168">Myself and others in 2024</a> for estimating the mean of random vectors.</li>
</ul>

<h1 id="a-general-variational-inequality">A general variational inequality</h1>

<p>Different authors use seemingly different PAC-Bayesian inequalities to achieve their results. However, <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2302.03421">we recently showed</a> that all of these inequalities are specific instantiations of the following more general result.</p>

<p>Let \(\Theta\) be some measurable parameter space and let \(N(\theta)\) be nonnegative and have expected value at most 1 (it’s an <a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/E-values">e-value</a>, if you like) for all \(\theta\in\Theta\). Then</p>

\[\Pr(\forall\rho\in\Mspace{\Theta}: \E_\rho \log N(\theta) \leq \kl(\rho\|\nu) + \log(1/\delta))\geq 1-\delta,\]

<p>where, as above, \(\Mspace{\Theta}\) is the set of all probability measures on \(\Theta\). The variational approach to concentration involves (i) finding some appropriate family of random variables \(N(\theta)\), (ii) choosing \(\nu\) and a family of distributions \(\{\rho_\theta:\theta\in\Theta\}\) such that \(\sup_\theta \kl(\rho_\theta \|\nu)\) is small, and (iii) applying this “master theorem.”</p>

<h1 id="example-1-sub-gaussian-random-vectors">Example 1: Sub-Gaussian random vectors</h1>

<p>This comes from our paper on <a rel="nofollow noopener noreferrer" href="https://arxiv.org/abs/2311.08168">time-uniform confidence spheres</a>. Consider \(n\) iid copies \(X_1,\dots,X_n\) of a \(\Sigma\)-sub-Gaussian random vector \(X\in\Re^d\). That is,</p>

\[\E\exp(\lambda \la \theta, X\ra) \leq \exp\left(\frac{\lambda^2}{2}\la\theta,\Sigma\theta\ra\right),\]

<p>for all \(\lambda\in\Re\) and \(\theta\in\Re^d\). This implies that</p>

\[N(\theta) = \exp\left\{\lambda\sum_{i\leq n}\la \theta, X_i\ra - \frac{n\lambda^2}{2}\la\theta,\Sigma\theta\ra\right\},\]

<p>has expectation at most 1. Let \(\nu\) be a Gaussian with mean 0 and covariance \(\beta^{-1}I\) for some \(\beta&gt;0\). Consider the family of distributions \(\{\rho_u:\norm{u}=1\}\) where \(\rho_u\) is a Gaussian with mean \(u\) and covariance \(\beta^{-1}I\). Then the KL divergence between \(\rho_u\) and \(\nu\) is \(\kl(\rho_u\|\nu) = \beta/2\). Using the master theorem above, we obtain that, with probability \(1-\delta\), <em>simultaneously for all distributions \(\rho\)</em>,</p>

\[\lambda\sum_{i\leq n} \E_\rho \la\theta, X_i\ra \leq \frac{n\lambda^2}{2}\E_\rho \la \theta, \Sigma\theta\ra + \frac{\beta}{2} + \log(1/\delta).\]

<p>Now, for \(\rho=\rho_u\), \(\E_\rho \la \theta, X_i\ra = \la u,X_i\ra\) and</p>

\[\E_\rho \la \theta, \Sigma\theta\ra = \la u,\Sigma u\ra + \beta^{-1}\Tr(\Sigma) \leq \norm{\Sigma} + \beta^{-1}\Tr(\Sigma),\]

<p>using basic properties of the expectation of quadratic forms under Gaussian distributions (see e.g. <a rel="nofollow noopener noreferrer" href="https://statproofbook.github.io/P/mean-qf.html">here</a>), and definition of the operator norm as \(\norm{A} = \sup_{u,v:\norm{u}=\norm{v}=1}\la u,\Sigma v \ra\). Since this holds simultaneously for all \(\rho_u\), we obtain that, with probability \(1-\delta\),</p>

\[\sup_u \lambda \sum_{i\leq n} \la u,X_i\ra \leq \frac{n\lambda^2}{2}(\norm{\Sigma} + \beta^{-1}\Tr(\Sigma)) + \frac{\beta}{2} + \log(1/\delta).\]

<p>The left hand side is equal to \(\lambda \norm{\sum_{i\leq n}X_i}\), which gives us our concentration result. One can then optimize \(\lambda\) using some calculus. This gives us state-of-the-art concentration up to an additive factor of \((\Tr(\Sigma^2)/n)^{1/4}\).</p>

<h1 id="example-2-random-matrices-with-finite-orlicz-norm">Example 2: Random matrices with finite Orlicz-norm</h1>

<p>This example is adapted from <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2108.08198">Zhivotovskiy (2024)</a>. Let \(M_1,\dots,M_n\) be iid copies of a random matrix \(M\) with finite sub-exponential <a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Orlicz_space">Orlicz norm</a>, in the sense that, for some \(C&gt;0\),</p>

\[\norm{\la \theta, M\phi\ra}_{\psi_1} \leq C \la\theta, \Sigma\phi\ra,\]

<p>for all \(\theta, \phi\in\Re^d\) where \(\Sigma = \E M\). Here</p>

\[\norm{Y}_{\psi_1} = \inf\left\{u&gt;0: \E\exp(|Y|/u)\leq 2\right\}.\]

<p>We take our parameter space in the master theorem above to be \(\Theta = \Re^d\times \Re^d\). Let \(\nu\) again be Gaussian with mean 0 and covariance \(\beta^{-1}\Sigma\) and let \(\mu_u\) be a <em>truncated</em> Gaussian mean \(u\), covariance \(\beta^{-1}\Sigma\) and radius \(r\). Being slightly loose with notation and writing \(\d\mu\) for the density of \(\mu\), the density of the truncated Gaussian can be written as</p>

\[\d\mu_{u}(x) = \frac{\ind\{\norm{x - u}\leq r\}}{Z}\d \rho_u,\]

<p>where \(Z\) is some normalizing constant and \(\rho_u\) is a non-truncated Gaussian. For a vector \(u\in \Sigma^{1/2}\mathbb{S}^{d-1}\), the KL-divergence between a truncated normal \(\mu_u\) and \(\nu\) is therefore</p>

\[\begin{align*}
    \kl(\mu_{u} \| \nu) &amp;= \int\log\left(\frac{1}{Z}\frac{\d \rho_u}{\d\nu}(\theta)\right) {\mu}_{u}(\d\theta) \\ 
    &amp;= \log\left(\frac{1}{Z}\right) + \frac{1}{2}\int (\la \theta-u,\beta\Sigma^{-1}(\theta-u)\ra + \la  \theta, \beta\Sigma^{-1}\theta\ra )\mu_{u}(\d\theta) \\ 
    &amp;= \log\left(\frac{1}{Z}\right) + \frac{\beta}{2}\int (2\la \theta,\Sigma^{-1}u\ra - \la  u, \Sigma^{-1}u\ra )\mu_{u}(\d\theta) \\ 
    &amp;= \log\left(\frac{1}{Z}\right) + \frac{\beta\la u,\Sigma^{-1} u\ra}{2} \leq  \log\left(\frac{1}{Z}\right) + \frac{\beta}{2}. 
\end{align*}\]

<p>Here \(Z = \Pr(\norm{\theta - u}\leq r)\) where \(\theta\sim \rho_{u}\). Equivalently, \(Z=\Pr(\norm{Y}\leq r)\) where \(Y\) is a normal with mean \(0\) and covariance \(\beta^{-1}\Sigma\). Hence \(1 - Z = \Pr(\norm{Y}&gt;r)\leq \E\norm{Y}^2/r^2 = \beta^{-1}\Tr(\Sigma)/r^2\). Thus, taking \(r = \sqrt{2 \beta^{-1}\Tr(\Sigma)}\) yields \(Z\geq 1/2\), and we obtain</p>

\[\begin{align}
\kl(\mu_u\|\nu) \leq \log(2) + \frac{\beta}{2}. 
\end{align}\]

<p>Therefore, since the KL divergence is additive on product measures, we have that</p>

\[\kl(\rho_u\times\rho_v\|\nu\times\nu) \leq 2\log(2) + \beta.\]

<p>Now it remains to construct a relevant quantity to use in the PAC-Bayes theorem. Consider</p>

\[N(\theta,\phi) = \exp\left\{\lambda\sum_{i\leq n}\la\theta, \Sigma^{-1/2}M_i\Sigma^{-1/2}\phi\ra - n\log\E\exp(\lambda\la \theta, \Sigma^{-1/2}M\Sigma^{-1/2}\phi\ra)\right\},\]

<p>where the expectation is over \(M\). It’s easy to see that this has expectation at most 1 (it can be written as the product of terms each with expectation exactly one). We apply the master theorem with the product distribution \(\mu_u\times \mu_v\) for \(u,v\in\Sigma^{1/2}\sd\) where \(\sd = \{x:\norm{x}=1\}\) is the unit sphere. Therefore, \(u = \Sigma^{1/2}u'\) for \(v = \Sigma^{1/2}v'\) for some \(u',v'\in\sd\). We obtain that with probability \(1-\delta\), for all \(u,v\),</p>

\[\begin{align}
&amp;\lambda \sum_{i\leq n}\E_{\mu_u\times \mu_v} \la \theta, \Sigma^{-1/2}M_i\Sigma^{-1/2}\phi\ra \\
&amp;\leq n \E_{\mu_u\times\mu_v} \log \E\exp(\lambda\la\theta, 
\Sigma^{-1/2}M\Sigma^{1/2}\phi\ra) + \frac{\beta}{2} + \log(2/\delta).\label{eq:matrix_bound1}\tag{2}
\end{align}\]

<p>The truncated Gaussian is symmetric about its mean, so the left hand side above becomes</p>

\[\E_{\mu_u\times\mu_v}\la \theta, \Sigma^{-1/2}M_i\Sigma^{-1/2}\phi\ra = \la \Sigma^{-1/2}u,M_i\Sigma^{-1/2}v\ra = \la u', M_i v'\ra,\]

<p>where, as before, \(u',v'\in\mathbb{S}^{d-1}\).  It remains to bound the right hand side of \eqref{eq:matrix_bound1}. For this we appeal to a result which bounds the MGF of a random variable in terms of its \(\psi_1\)-norm. In particular, we appeal to an <a href="/research_notes/exponential_inequalities/">exponential inequality</a>, which states that for a random variable \(Y\),</p>

\[\begin{equation}
\label{eq:exp_ineq}
\E[\exp(\lambda (Y - \E Y))]\leq \exp(4\lambda^2\norm{Y-\E Y}_{\psi_1}^2),\quad \forall |\lambda| \leq \frac{1}{2\norm{Y-\E Y}_{\psi_1}}.\tag{3}
\end{equation}\]

<p>Applying this with \(Y = \la \theta,\Sigma^{-1/2} M\Sigma^{-1/2}\phi\ra\) and noting that \(\E Y = \la \theta, \phi\ra\), we have</p>

\[\begin{align}
&amp; \norm{\la \theta, \Sigma^{-1/2}M \Sigma^{-1/2} \phi\ra - \la \theta, \Sigma^{-1/2}\E M \Sigma^{1/2} \phi\ra}_{\psi_1} \\ 
&amp;\leq \norm{\la \theta, \Sigma^{-1/2}M \Sigma^{-1/2} \phi\ra}_{\psi_1} + \norm{\la \theta, \Sigma^{-1/2}\E M \Sigma^{-1/2} \phi\ra}_{\psi_1} \\ 
&amp;\leq \norm{\la \theta, \Sigma^{-1/2}M \Sigma^{-1/2} \phi\ra}_{\psi_1} + \E\norm{\la \theta, \Sigma^{-1/2} \Sigma^{-1/2} \phi\ra}_{\psi_1} \\ 
&amp;= 2 \norm{\la \Sigma^{-1/2}\theta, M \Sigma^{-1/2} \phi\ra}_{\psi_1} \\ 
&amp;\leq 2C \la \Sigma^{-1/2}\theta, \Sigma\Sigma^{-1/2}\phi\ra = 2C\la \theta, \phi\ra \leq C(\norm{\theta}^2 + \norm{\phi}^2). 
\end{align}\]

<p>Therefore, \eqref{eq:exp_ineq} yields</p>

\[\begin{align}
\E\exp(\lambda \la\theta,\Sigma^{-1/2}M\Sigma^{-1/2}\phi\ra) &amp;\leq \exp(\lambda \la \theta, \phi\ra + 4C^2\lambda^2(\norm{\theta}^2 + \norm{\phi}^2)^2). \label{eq:Eexp} \tag{4}
\end{align}\]

<p>Note that \(\norm{\theta - u}\leq r\) and \(\norm{u} = \norm{\Sigma^{1/2}u'} \leq \norm{\Sigma^{1/2}} \leq \sqrt{\norm{\Sigma}}\), so</p>

\[\norm{\theta}^2 \leq (r + \norm{u})^2 \leq \left(\sqrt{2\beta^{-1}\Tr(\Sigma)} + \sqrt{\norm{\Sigma}}\right)^2.\]

<p>The same bound holds for \(\norm{\phi}^2\). Therefore, \eqref{eq:Eexp} gives</p>

\[\begin{align}
&amp;\E_{\rho_u\times\rho_v} \log \E \exp(\lambda\la\theta, \Sigma^{-1/2} M\Sigma^{-1/2}\phi\ra ) \\
&amp;\leq \lambda \la u,v\ra + 8C^2\lambda^2\left(\sqrt{2\beta^{-1}\Tr(\Sigma)} + \sqrt{\norm{\Sigma}}\right)^4 \\ 
&amp;= \lambda \la u',\Sigma v'\ra + 8C^2\lambda^2\left(\sqrt{2\beta^{-1}\Tr(\Sigma)} + \sqrt{\norm{\Sigma}}\right)^4,
\end{align}\]

<p>assuming that</p>

\[|\lambda| \leq \frac{1}{4C(\sqrt{2\beta^{-1}\Tr(\Sigma)} + \sqrt{\norm{\Sigma}})^2}.\]

<p>Choosing \(\beta = 2\Tr(\Sigma)/\norm{\Sigma}\) and putting everything together, \eqref{eq:matrix_bound1} gives that with probability \(1-\delta,\)</p>

\[\sup_{u',v'\in\sd}\lambda\sum_{i\leq n} \la u', (M_i-\Sigma) v'\ra \lesssim C^2\lambda^2 \norm{\Sigma}^2 + \frac{\Tr(\Sigma)}{\norm{\Sigma}} + \log(1/\delta).\]

<p>Dividing by \(n\) and optimizing over \(\lambda\) gives us a final bound that matches state-of-the-art concentration bounds for random matrices.</p>


  <small>
    <a link="back", href="/research_notes/">Back to all notes</a>
  </small>
</article>
 <!-- "> -->
      </main>

      <!-- Footer  -->
      <div class="container" id="footer">
        <hr>
        <div class='container links'>
          <a href="https://github.com/bchugg" rel='nofollow'><img src="/assets/images/github.png"></a>
          <a href="https://www.goodreads.com/user/show/90945992-ben-chugg" rel='nofollow'><img src="/assets/images/goodreads.png"></a>
          <a href="https://twitter.com/BennyChugg" rel='nofollow'><img src="/assets/images/twitter.png"></a>
        </div>
      
        <div class='copyright'>
          <small>&#169; 2025 Ben Chugg</small>
        </div>
      
      </div>
    </div>

  </body>
</html> 
