<!DOCTYPE html>
<html lang="en-us">

  <head prefix="og: http://ogp.me/ns#; dc: http://purl.org/dc/terms/#">
  
  
  <!-- Canonical link to help search engines -->
  <link rel="canonical" href="http://localhost:4000/research_notes/rl_vs_pps/" />

  <!-- Basic meta elements -->
  <meta charset="utf-8" />

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width,initial-scale=1.0,shrink-to-fit=no" />

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <title>
    
     OPE in RL vs Mean Estimation in Survey Sampling
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="OPE in RL vs Mean Estimation in Survey Sampling" />
<meta name="author" content="Ben Chugg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A link between reinforcement learning and survey sampling" />
<meta property="og:description" content="A link between reinforcement learning and survey sampling" />
<link rel="canonical" href="http://localhost:4000/research_notes/rl_vs_pps/" />
<meta property="og:url" content="http://localhost:4000/research_notes/rl_vs_pps/" />
<meta property="og:site_name" content="benny" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-25T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="OPE in RL vs Mean Estimation in Survey Sampling" />
<meta name="twitter:site" content="@bennychugg" />
<meta name="twitter:creator" content="@Ben Chugg" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Chugg","url":"https://benchugg.com"},"dateModified":"2022-05-25T00:00:00-07:00","datePublished":"2022-05-25T00:00:00-07:00","description":"A link between reinforcement learning and survey sampling","headline":"OPE in RL vs Mean Estimation in Survey Sampling","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research_notes/rl_vs_pps/"},"url":"http://localhost:4000/research_notes/rl_vs_pps/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Dublin Core metadata for Zotero -->
  <meta property="dc:title" content="OPE in RL vs Mean Estimation in Survey Sampling" />
  <meta property="dc:creator" content="Ben Chugg" />
  <meta property="dc:identifier" content="http://localhost:4000/research_notes/rl_vs_pps/" />
  
  <meta property="dc:date" content="2022-05-25 00:00:00 -0700" />
  
  <meta property="dc:source" content="benny" />

  <!-- Open Graph and Twitter metadata -->
  <meta property="og:title" content="OPE in RL vs Mean Estimation in Survey Sampling" />
  <meta property="og:url" content="http://localhost:4000/research_notes/rl_vs_pps/" />
  
    <meta property="og:image" content="/assets/images/heads.jpeg" />
    <meta name="twitter:image" content="/assets/images/heads.jpeg" />
  
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:domain" value="benchugg.com" />
  <meta name="twitter:title" value="OPE in RL vs Mean Estimation in Survey Sampling" />
  <meta name="twitter:url" value="http://localhost:4000" />
  
  <!-- Description is dependent on page type  -->
  
    <meta property="og:description" content="A link between reinforcement learning and survey sampling">
    <meta name="twitter:description" value="A link between reinforcement learning and survey sampling" />
    <meta property="og:type" content="article" />
  
  

  <!-- CSS link -->
  <link rel="stylesheet" href="/assets/css/style.css" />

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="167x167" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="180x180" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon.ico">
  <link rel="shortcut icon" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="114×114" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="72×72" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” href="/assets/favicon.ico" />
  <link rel=”icon” href="/assets/favicon.ico", sizes="32x32"/>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml" />

  
</head>


  <body class="theme-base-">

    <!-- This if statement decides which sidebar to use -->
    
    <!--
  Target for toggling the sidebar `.sidebar-checkbox` is for regular styles, `#sidebar-checkbox` for
  behavior.
-->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<aside class="sidebar" id="sidebar">
  <div class="sidebar-item">
    
  </div> 

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
    
      
    
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/papers/">Papers</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/research_notes/">Research Notes</a>
        
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/writing/">Writing</a>
        
      
    

    <a class='sidebar-nav-item' href="http://incrementspodcast.com">Podcast</a>

    <span class='sidebar-nav-item'>Recommendations</span>
    <ul class="dropdown-menu">
      
        
          
        
      
        
      
        
      
        
      
        
      
        
          
            <li><a href="/books/">Books</a></li>
          
        
      
        
          
            <li><a href="/courses/">Courses</a></li>
          
        
      
        
          
        
      
        
          
        
      
        
      
        
      
        
          
        
      
    </ul>

   </nav> 

  <div class="sidebar-links">
    <div class='container'>
      <a href="https://github.com/bchugg" rel='nofollow'><img src="/assets/images/github_white.png"></a>
      <a href="https://www.linkedin.com/in/ben-chugg-3a4616aa/" rel='nofollow'><img src="/assets/images/linkedin_white.png"></a>
      <a href="https://www.goodreads.com/user/show/90945992-ben-chugg" rel='nofollow'><img src="/assets/images/goodreads_white.png"></a>
      <a href="https://twitter.com/BennyChugg" rel='nofollow'><img src="/assets/images/twitter_white.png"></a>
    </div>
  </div>
</aside>

    

    <!--
      Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS
      collisions with our real content.
    -->
    <div class="wrap">
      <header class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">benny</a>
            
          </h3>
        </div>
      </header>

      <main class="container content" id="main">
        <article class="note">
  <small>
    <a href="/research_notes/">Back to all notes</a>
  </small>
  <hr>
  <p class='title'>OPE in RL vs Mean Estimation in Survey Sampling</p> 
  \[\newcommand{\hgamma}{\hat{\gamma}}
\newcommand{\H}{\mathcal{H}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\S}{\mathcal{S}}
\newcommand{\hmu}{\hat{\mu}}
\newcommand{\hmuht}{\hmu_{\bf NHT}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand{\hmuha}{\hat{\mu}_{\bf Háyek}}
\newcommand{\hmudr}{\hmu_{\bf DR}}
\newcommand{\model}{\hat{f}}
\newcommand{R}{\mathbb{R}}
\newcommand{\hy}{\hat{y}}
\newcommand{\hpi}{\hat{\pi}}
\newcommand{\hmum}{\hmu_{\bf Model}}
\newcommand{\hmuipw}{\hmu_{\bf IPW}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\by}{\bar{y}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\hV}{\widehat{V}}
\newcommand{\hVm}{\hV_{\bf Model}}
\newcommand{\hVipw}{\hV_{\bf IPW}}
\newcommand{\hVdr}{\hV_{\bf DR}}\]

<ul id="markdown-toc">
  <li><a href="#1-rl-perspective" id="markdown-toc-1-rl-perspective">1. RL Perspective</a>    <ul>
      <li><a href="#11-model-based-estimator-hvm" id="markdown-toc-11-model-based-estimator-hvm">1.1 Model Based Estimator, \(\hVm\)</a></li>
      <li><a href="#12-inverse-propensity-weighting-hmuipw" id="markdown-toc-12-inverse-propensity-weighting-hmuipw">1.2 Inverse Propensity Weighting, \(\hmuipw\)</a></li>
      <li><a href="#13-doubly-robust-estimator-hvdr" id="markdown-toc-13-doubly-robust-estimator-hvdr">1.3 Doubly Robust Estimator, \(\hVdr\)</a></li>
      <li><a href="#14-upshot" id="markdown-toc-14-upshot">1.4 Upshot</a></li>
    </ul>
  </li>
  <li><a href="#2-survey-sampling-perspective" id="markdown-toc-2-survey-sampling-perspective">2. Survey Sampling Perspective</a>    <ul>
      <li><a href="#21-inclusion-probabilities" id="markdown-toc-21-inclusion-probabilities">2.1 Inclusion Probabilities</a></li>
      <li><a href="#22-variance-calculations" id="markdown-toc-22-variance-calculations">2.2 Variance Calculations</a></li>
    </ul>
  </li>
</ul>

<p>Off Policy Evaluation (OPE) in Reinforement learning and mean estimation in survey sampling are distinct problems and settings, but can employ similar tools. We’ll investigate model-based estimates, inverse propensity weighting, and doubly robust estimation in the context of both problems.</p>

<h1 id="1-rl-perspective">1. RL Perspective</h1>

<p>The setting is given by a traditional contextual bandit setup, wherein each timestep some context vector is observed, a policy takes an action from a pre-specified and finite action set \(\A\), and the resulting reward is revealed. The rewards pertaining to all other untaken actions are not observed.</p>

<p>More specifically, repeat for \(i=1,\dots,N\):</p>
<ul>
  <li>\(x_i\in\R^d\) drawn iid from some (unknown) distribution \(\D\). Call \(x_i\) the <em>context</em> or <em>covariates</em>.</li>
  <li>Some (unknown) policy \(\gamma\) takes action \(a_i\in\A\), possibly depending on history contexts, actions, and rewards to this point. Write \(\gamma_i(a\vert x_i)\), for the probability that \(\gamma\) selects action \(a\), where the index \(i\) allows for possibility that \(\gamma\) is a function of the past. Let</li>
  <li>Reward \(y_i\in\R\) is drawn from \(\D(\cdot\vert x_i,a_i)\) and revealed to the policy.</li>
</ul>

<p>The goal of policy evaluation is to use the data generated in this way to evaluate the expected reward of a policy \(\pi\), most likely distinct from \(\gamma\). Formally, if</p>

\[f(x,a) = \E_{y\sim \D(\cdot\vert x,a)}[y],\]

<p>we want to estimate</p>

\[\begin{equation}
\label{eq:Vpi}
V(\pi) = \E_{x\sim \D}\E_{a\sim \pi(\cdot\vert x)}[f(x,a)]. \tag{1}
\end{equation}\]

<p>For simplicitly, we’ll assume that \(\pi\) is <em>stationary</em>, i.e., it doesn’t change over time. The results are similar if we don’t make this assumption, but somewhat harder to interpret as their is an extra sum in all the equations.</p>

<p>Also, instead of writing out three expectations everytime, we’ll often write \(\E_{\pi}\) to indicate that the expected value in Equation \eqref{eq:Vpi} depends on the policy, i.e., \(V(\pi)=\E_{\pi}[y].\)</p>

<p>So, how would you estimate this? The empirical mean of the select rewards won’t do since the actions that gave rise to them were selected by \(\gamma\), not \(\pi\). This leads us to modelling the rewards directly.</p>

<h2 id="11-model-based-estimator-hvm">1.1 Model Based Estimator, \(\hVm\)</h2>

<p>One natural approach is to train a model to predict the reward given the context and action. Given such a model \(\model:\A\times\R^d\to\R\), we can just use the predicted mean of all the data:</p>

\[\hVm = \frac{1}{N}\sum_{k} \sum_{a\in\A}\pi(a\vert x_k)\model(a,x_k).\]

<p>Clearly, \(\hVm\) is only as good as \(\model\). To see this more quantitatively,define</p>

\[\Delta(a,x)=\model(a,x) - f(a,x),\]

<p>i.e., the difference between the model’s estimate of the reward and the true expected reward. Then the expected bias of \(\hVm\) is</p>

\[\begin{align}
\E_{\gamma}[\hVm] &amp;= \sum_{a\in\A} \E_\gamma[\pi(a|x)(\Delta(a,x) + f(a,x))] \\
&amp;= \sum_{a\in\A} \bigg(\E_x[\pi(a\vert x)\Delta(a,x)] + \E_x[\pi(a\vert x)f(a,x))]\bigg) \\ 
&amp;= \bigg(\E_{x,a\sim\pi}[\Delta(a,x)] + \E_{x,a\sim\pi}[f(a,x))]\bigg) \\ 
&amp;= \E_{x,a\sim \pi}[\Delta(a,x)] + V(\pi),
\end{align}\]

<p>where we’ve used stationarity of \(\pi\) to remove the sum over \(k\). Note also that we’re taking the expectation w.r.t. \(\gamma\), not \(\pi\). This is because we know what \(\pi\) is – we’re trying to understand the behavior of our estimates under the uncertainty induced by \(\D\) and \(\gamma\).</p>

<p>Thus, we see that \(\hVm\) is biased by a linear factor of \(\Delta\). We get a similar story with the variance. Since each \(x\) is drawn iid from \(\D\), we can simply sum the variance across each term to get:</p>

\[\begin{align}
\Var_\gamma(\hVm) &amp;= \frac{1}{N^2}\sum_k \Var_{x}\bigg[\sum_{a\in\A}\pi(a|x)\model(a,x)\bigg] \\ 
&amp;= \frac{1}{N}\Var_{x}\bigg[\sum_{a\in\A}\pi(a|x)\model(a,x)\bigg] \\ 
&amp;= \frac{1}{N}\Var_{x}\E_{a\sim \pi}[\model(a,x)]\\ 
&amp;= \frac{1}{N}\Var_{x}\E_{a\sim \pi}[\Delta(a,x) + f(a,x)]\\ 
\end{align}\]

<p>The quantity \(N^{-1}\Var_x\E_{a\sim\pi}[f(a,x)]\) is unavoidable variance obtained from sampling according to \(\pi\). So we see that both the bias and variance of \(\hVm\) depend on a very natural way on the error of model.</p>

<h2 id="12-inverse-propensity-weighting-hmuipw">1.2 Inverse Propensity Weighting, \(\hmuipw\)</h2>

<p>If we’re not confident in our ability to obtain a good model of \(f(x,a)\) (likely in practice, since \(\pi\) is most likely interested in different regions of the space than \(\gamma\) - otherwise what are we doing), then we go try instead to estimate the selection probability under \(\gamma\). If \(\hgamma(a\vert x)\) is an estimate of \(\gamma(a\vert x)\), then we can use inverse propensity weighting (IPW):</p>

\[\hVipw = \frac{1}{N}\sum_{k}\frac{\pi(a_k|x_k)}{\hgamma(a_k\vert x_k)}y_k.\]

<p>The idea driving the IPW estimator is to start with the empirical mean of the rewards weighted by their selection probability under \(\pi\). If rewards were selected at uniformly at random, then this estimator (\(N^{-1}\sum\pi(a_k\vert x_k)y_k\)) would be unbiased. Since they’re (probably) not, we have to correct this estimate by dividing by the selection probability under \(\gamma\). But since we don’t have access to \(\gamma\) we have to use \(\hgamma\).</p>

<p>In practice, it is typically easier to get an estimate of \(\gamma\) than it is to train a reliable \(\model\), so the IPW estimator is usually preferred over the model-based estimator.</p>

<p>In order to examine the bias of \(\hVipw\), define</p>

\[\lambda(a,x) = \frac{\gamma(a,x)}{\hgamma(a,x)},\]

<p>to be the ratio of the true policy selection probability and our estimate. Then,</p>

\[\begin{align}
\E_{\gamma}[\hVipw] &amp;= \E_x\E_{a\sim \gamma}\bigg[\frac{\pi(a\vert x)}{\hgamma(a\vert x)}\E_y[y_k|x,a]\bigg] \\ 
&amp;= \E_x\bigg[\sum_{a\in \A}\frac{\pi(a\vert x)}{\hgamma(a\vert x)}f(a,x)\gamma(a\vert x)\bigg] \\ 
&amp;= \E_x\bigg[\sum_{a\in\A}\lambda(a,x)\pi(a,x)f(a,x)\bigg] \\ 
&amp;= \E_{x,a\sim\pi}[\lambda(a,x)f(a,x)\bigg]. 
\end{align}\]

<p>Therefore the bias is</p>

\[\vert \E_\gamma[\hVipw] - V(\pi)\vert| = \E_{\pi}[f(x,a)(1-\lambda(a,x))].\]

<p>That is, if \(\gamma=\hgamma\) and \(\lambda\equiv 1\), then the IPW estimator is unbiased. Instead of computing the variance directly, we’ll use the fact that the next estimator is a generalization of \(\hVipw\) to obtain it indirectly.</p>

<h2 id="13-doubly-robust-estimator-hvdr">1.3 Doubly Robust Estimator, \(\hVdr\)</h2>

<p>Finally, we come to the <em>doubly robust</em> estimator, which combines the strengths of the previous estimators. Let</p>

\[F(x) = \sum_{a\in\A}\pi(a\vert x)\model(a,x),\]

<p>be the model estimate for a particular context \(x\). The DR estimator is</p>

\[\begin{align}
\hVdr &amp;= \frac{1}{N}\sum_k \bigg(F(x_k) + \frac{\pi(a_k\vert x_k)}{\hgamma(a_k\vert x_k)}(y_k-\model(a_k,x_k))\bigg).
\end{align}\]

<p>In words, the doubly robust estimator relies on the model’s predicted reward, but corrects the model when the true reward (\(y_k\)) is available. It can be helpful to observe that</p>

\[\begin{equation}
\label{eq:vdr2}
\hVdr = \hVm + \hVipw - \frac{1}{N}\sum_{k}\frac{\pi(a_k\vert x_k)}{\hgamma(a_k\vert x_k)}\model(a_k,x_k). \tag{2}
\end{equation}\]

<p>At first glance, this estimator seems a little unwieldy. However, it’s useful because it is unbiased whenever either \(\Delta\equiv0\) <em>or</em> \(\lambda\equiv0\). To see this, note that the last term in \eqref{eq:vdr2} has expectation</p>

\[\begin{align}
\E_\gamma\bigg[\frac{\pi(a\vert x)}{\hgamma(a\vert x)}(\Delta(a,x)+f(a,x))\bigg] &amp;= \E_x\bigg[\sum_a \pi(a\vert x)(\Delta(a,x)+f(a,x))\lambda(a,x)\bigg] \\ 
&amp;= \E_{\pi}[\lambda(a,x)(\Delta(a,x)+f(a,x))]. 
\end{align}\]

<p>From this and the bias of \(\hVm\) and \(\hVipw\) it follows that</p>

\[\begin{align}
\E_\gamma[\hVdr] &amp;= \E_\pi[\Delta(a,x)] + V(\pi) + \E_\pi[\lambda(a,x)f(a,x)] - 
\E_\pi[\lambda(a,x)(\Delta(a,x)+f(a,x))] \\ 
&amp;= \E_\pi[\Delta(a,x)(1-\lambda(a,x))] + V(\pi),
\end{align}\]

<p>that is,</p>

\[\vert \E_\gamma[\hVdr] - V(\pi)\vert = \E_\pi[\Delta(a,x)(1-\lambda(a,x))],\]

<p>meaning that \(\hVdr\) is unbiased if either the model \(\model\) is accurate, or if the estimated policy \(\hgamma\) is accurate.</p>

<p>To compute the variance and keep things legible, we’ll drop the arguments \(x\) and \(a\) from most functions. Consider the second moment of,</p>

\[A_k = F(x_k) + \frac{\pi(a_k\vert x_k)}{\hgamma(a_k\vert x_k)}(y_k - \model(a_k,x_k)),\]

<p>a single term in the sum of \(\hVdr\). We’ll use the same trick as above to convert the expectation over \(\gamma\) into one over \(\pi\) (which, we recall is shorthand for the distribution \(x\sim\D,a\sim\pi(\cdot\vert x),y\sim\D(\cdot\vert x,a)\)):</p>

\[\begin{align}
\E_\gamma[A_k^2] &amp;= \E_x[F^2] + 2\E_\gamma\bigg[F\frac{\pi}{\hgamma}(y - \model)\bigg] + \E_\gamma\bigg[\frac{\pi^2}{\hgamma^2}(y - \model)^2\bigg] \\ 
&amp;= \E_x[F^2] + 2\E_\pi[F\lambda (y-\model)] + \E_\pi\bigg[\frac{\pi}{\hgamma}\lambda (y-\model)^2\bigg] \\ 
&amp;= \E_x[F^2] + 2\E_{x,a}[F\lambda (f-\model)] + \E_\pi\bigg[\frac{\pi}{\hgamma}\lambda (y-\model)^2\bigg] \\ 
&amp;= \E_{\pi}[(F - \lambda\Delta)^2] - \E_\pi[\lambda^2\Delta^2] + \E_\pi\bigg[\frac{\pi}{\hgamma}\lambda (y-\model)^2\bigg]. 
\end{align}\]

<p>Noticing from the calculations above that \(\E_\gamma[A_k] = \E_\pi[F-\lambda \Delta]\), this gives</p>

\[\Var_\gamma[A_k] = \E_\gamma[A_k^2] - \E_\gamma[A_k]^2 = \Var_\pi[F - \lambda\Delta] - \E_\pi[\lambda^2\Delta^2] + \E_\pi\bigg[\frac{\pi}{\hgamma}\lambda (y-\model)^2\bigg].\]

<p>Set</p>

\[B=\E_\pi\bigg[\frac{\pi}{\hgamma}\lambda (y-\model)^2\bigg],\]

<p>and notice that</p>

\[\begin{align}
B &amp;= \E_{x,a,r}[\pi\lambda\hgamma^{-1}(y^2 - 2y\model + \model^2)] \\ 
&amp;= \E_{x,a}[\pi\lambda\hgamma^{-1}(\E_{\D(\cdot\vert x,a)}[y^2] - 2f\model + \model^2)] \\ 
&amp;= \E_{x,a}[\pi\lambda\hgamma^{-1}(\E_{\D(\cdot\vert x,a)}[y^2] - f^2 + \Delta^2)] \\ 
&amp;= \E_{x,a}[\pi\lambda\hgamma^{-1}(\Var_{\D(\cdot\vert x,a)}[y] + \Delta^2)].
\end{align}\]

<p>Applying total variance,</p>

\[\begin{align}
\Var_\gamma[A_k] &amp;= \Var_x\E_{a\sim\pi}[F-\lambda\Delta] + \E_x\Var_{a\sim\pi}[F-\lambda\Delta] - \E_\pi[\lambda^2\Delta^2] + B \\ 
&amp;= \Var_x[F - \E_a\lambda\Delta] + \E_x\Var_a[\lambda\Delta] - \E_\pi[\lambda^2\Delta^2] + B  \\ 
&amp;=  \Var_x\E_{a\sim\pi}[\model - \lambda\Delta] + \E_x[\E_a[(\lambda\Delta)^2]] - \E_x[\E_a[\lambda\Delta]^2] - \E_\pi[\lambda^2\Delta^2] + B\\
&amp;= \Var_x\E_{a\sim\pi}[f + \Delta(1-\lambda)] - \E_x[\E_a[\lambda\Delta]^2] + B.
\end{align}\]

<p>Finally, putting everything together we get</p>

\[\begin{equation}
\label{eq:V_dr}
\Var_\gamma[\hVdr] = \frac{1}{N}\bigg(\Var_x\E_{a\sim\pi}[f + \Delta(1-\lambda)] - \E_x[\E_a[\lambda\Delta]^2] + \E_{x,a}[\frac{\pi\lambda}{\gamma}(\Var[y]+ \Delta^2)]\bigg).  \tag{3}
\end{equation}\]

<p>Noting that \(\hVipw\) is \(\hVdr\) with \(\model\equiv0\) (hence \(\Delta=-f\)), we also get the variance for the IPW estimator:</p>

\[\begin{equation}
\label{eq:V_ipw}
\Var_\gamma[\hVipw] = \frac{1}{N}\bigg(\Var_x\E_{a\sim\pi}[f\lambda] - \E_x[\E_a[f\lambda]^2] + \E_{x,a}[\frac{\pi\lambda}{\hgamma}(\Var_{\D(\cdot\vert x,a)}[y]+ f^2)]\bigg).  \tag{4}
\end{equation}\]

<h2 id="14-upshot">1.4 Upshot</h2>

<p>A key distinction between Equations \eqref{eq:V_dr} and \eqref{eq:V_ipw} is the third term. The variance of IPW depends on the true values \(f(a,x)\) whereas the variance of DR depends on the model residuals \(\Delta(a,x)\). Under a good model fit, therefore, the variance of the doubly robust estimator can be significantly smaller.</p>

<p>Consider what happens when either \(\Delta=0\) or \(\lambda=1\) (model is accurate or estimated selection probability is accurate). Then</p>

\[\Var(\hVipw) - \Var(\hVdr) = \frac{1}{N}\bigg(\E_{x,a}\bigg[\frac{\pi}{\gamma}f^2\bigg] - \E_x[\E_a[f]^2]\bigg).\]

<p>If \(\gamma=\hgamma\) is small with non-negligible probability, then this difference can be large.</p>

<p>On the other hand, the model based variance isn’t a function of the randomness in rewards (\(\Var[y]\) or the policy \(\gamma\), so can be much smaller in general. In practive, however, it’s often harder to generate a reliable model \(\model\) than a estimate of \(\gamma\).</p>

<h1 id="2-survey-sampling-perspective">2. Survey Sampling Perspective</h1>

<p>Let \([N]=\{1,\dots,N\}\) be a finite population of \(N\) distinct units. Each \(i\in X\) has a hidden value \(y_i\) and observable quantities \(x_i\in\R^d\). A sample \(S\subset [N]\) of size \(n\) has been drawn according to the <em>inclusion probabilities</em> \(\pi_i=\Pr(i\in S)\). The goal is to estimate the true mean of the population</p>

\[\begin{equation}
\label{eq:mean}
\mu = \frac{1}{N}\sum_{i\in [N]} y_i. \tag{5}
\end{equation}\]

<p>This is the equivalent to estimating average reward in RL, but the two settings are clearly quite distinct. For one, we’re not assuming a distribution over the covariates \(x_i\); they’re simply given. This will be important for variance calculations, since there’s no independence assumptions and we can’t ignore covariance. Second, we assume that the probabilities \(\pi_i\) are known. You can change this assumption without too much fuss, in which case the results here resemble the RL setting more closely. Finally, in survey sampling, we typically don’t assume that the hidden values are generated from a conditional distribution of the context. Instead, they’re deterministic given the covariates. All of these assumptions simplify the calculations.</p>

<p>Still, ignoring the details, the two settings can be tackled with similar approches. You can imagine training a model \(\model:\R^d\to\R\) to predict the reward given the covariates, and then estimating the mean by simply predicting over the population:</p>

\[\hmum = \frac{1}{N}\sum_{i\in[N]} \model(x_i).\]

<p>If \(\Delta(x_i) = \model(x_i) - y_i\), then the bias of \(\hmum\) is</p>

\[\E_\model[\hmum] = \frac{1}{N}\sum_i \E_\model[\Delta(x_i)] + \mu,\]

<p>which is very similar to the RL setting. Note the only randomness is any randomness in the model.</p>

<p>The IPW estimator (also called the Narain-Horvitz-Thompson estimator in sampling theory) is</p>

\[\begin{equation}
\hmuipw = \frac{1}{N}\sum_{i\in [N]} \frac{y_i}{\pi_i}\delta_S(i), 
\end{equation}\]

<p>where \(\delta_S(i)=1\) if \(i\in S\) and 0 otherwise. Since \(\E[\delta_S(i)] = \pi_i\), it’s easy to see that \(\E[\hmuht] = \mu\). If we didn’t know the inclusion probabilities then we could use estimates \(\hpi\) and we’d obtain a bias which depends multiplicatively on the ratio \(\pi/\hpi\) as above.</p>

<p>The DR estimator uses both the model predictions and the observed rewards:</p>

\[\begin{equation}
\hmudr = \frac{1}{N}\sum_{i\in [N]}\bigg(\frac{y_i-\model(x_i)}{\pi_i}\delta_S(i) + \model(x_i)\bigg).
\end{equation}\]

<p>In the survey sampling setting, the intuition behind the DR estimator is even clearer. Essentially, we’re relying on the model estimates for those elements we didn’t sample and correcting the estimate where we have the true label. As with the IPW estimator, it’s easy to see that \(\hmudr\) is unbiased.</p>

<h2 id="21-inclusion-probabilities">2.1 Inclusion Probabilities</h2>

<p>A brief aside on the probabilities \(\pi_i\). Let \(\S=\{S_1,\dots,S_{N\choose n}\}\) be the set of all possible sets of sampled elements. Note that \(\Pr=\Pr_\S\) is a measure on \(\S\). 
Inclusion probabilities do not behave as a typical distribution because they are defined over multiple rounds of sampling. In particular, \(\sum_i \pi_i \neq 1\). Instead,</p>

\[\sum_i \pi_i = \sum_i \sum_{S\in\S} \Pr(S) \delta_S(i) = \sum_{S\in \S}\Pr(S)\sum_i \delta_S(i) = n\sum_{S\in \S}\Pr (S) = n,\]

<p>since \(\vert S\vert=n\) for each \(S\in\S\). Here \(\delta_S(i)\) indicates whether \(i\in S\).</p>

<p>We can also define higher order inclusion probabilities. For \(i\neq j\), let \(\pi_{i,j}=\Pr(i,j\in S)\), and so on.</p>

<h2 id="22-variance-calculations">2.2 Variance Calculations</h2>

<p>Put</p>

\[A(z) = \sum_{i\in [N]} \frac{z_i}{\pi_i}\delta_S(i).\]

<p>Then</p>

\[\begin{align}
\Var(A(z)) &amp;= 
\sum_{i\in [N]}\frac{z_i^2}{\pi_i^2}\Var(\delta_S(i)) + \sum_{i\in [N]}\sum_{j\neq i}\frac{z_iz_j}{\pi_i\pi_j}\Cov(\delta_S(i),\delta_S(j)) \\
&amp;= \sum_{i\in [N]}\frac{z_i^2}{\pi_i}(1-\pi_i) + \sum_{i\in [N]}\sum_{j\neq i}\frac{z_iz_j}{\pi_i \pi_j}(\pi_{i,j}-\pi_i\pi_j), 
\end{align}\]

<p>since \(\Var(\delta_S(i)) = \E[\delta_S(i)] - \E[\delta_S(i)]^2 = \pi_i(1-\pi_i)\) and similarly for the covariance. (Note that \(\delta_S(i)=\delta_S(i)^2\).) Now, note that sampling one set of \(n\) elements is a disjoint event from sampling another. Thus we can write</p>

\[\begin{align}
\sum_{j\neq i}\pi_{i,j}&amp;=\sum_{j\neq i}\sum_S \Pr(S)\delta_S(i)\delta_S(j)=\sum_{S:i\in S}\Pr(S)\sum_{j\neq i}\delta_S(j)\\
&amp;= (n-1)\sum_S \Pr(S)\delta_S(i) = (n-1)\pi_i.
\end{align}\]

<p>Consequently,</p>

\[\sum_{j\neq i}(\pi_{i,j}-\pi_i\pi_j) = (n-1)\pi_i - \pi_i(n-\pi_i) = \pi_i(\pi_i-1).\]

<p>Plugging this into the first sum of \(\Var(A(z))\) we have</p>

\[\begin{align}
\Var(A(z)) &amp;= -\sum_{i\in [N]}\frac{z_i^2}{\pi_i^2}\sum_{j\neq i}(\pi_{i,j}-\pi_i\pi_j)+ \sum_{i\in [N]}\sum_{j\neq i}\frac{z_iz_j}{\pi_i \pi_j}(\pi_{i,j}-\pi_i\pi_j)\\
&amp;= \sum_{i\in [N]}\sum_{j\neq i}\bigg(\frac{z_iz_j}{\pi_i \pi_j}-\frac{z_i^2}{\pi_i^2}\bigg)(\pi_{i,j}-\pi_i\pi_j) \\ 
&amp;= \frac{1}{2}\sum_{i\in [N]}\sum_{j\neq i}\bigg(\frac{z_i}{\pi_i}-\frac{z_j}{\pi_j}\bigg)^2(\pi_i\pi_j-\pi_{i,j}).
\end{align}\]

<p>In the last line we completed the square and divided by two to account for the extra terms being introduced.</p>

<p>Using this, we can write out the variance of both the NHT and doubly-robust estimators:</p>

\[\Var(\hmuht) = \frac{1}{2N^2}\bigg\{\sum_{i,j\in [N]}\bigg(\frac{y_i}{\pi_i}-\frac{y_j}{\pi_j}\bigg)^2(\pi_i\pi_j-\pi_{i,j})\bigg\},\]

<p>and</p>

\[\Var(\hmudr) = \frac{1}{2N^2}\bigg\{\sum_{i,j\in [N]}\bigg(\frac{y_i-\hy_i}{\pi_i}-\frac{y_j-\hy_j}{\pi_j}\bigg)^2(\pi_i\pi_j-\pi_{i,j})\bigg\}.\]

<p>These look similar, but the squared terms differ quite drastically in their behaviour. First, notice that \(\Var(A(z))=0\) if \(z_i/\pi_i\) is constant for every \(i\). For the NHT estimator, \(z_i=y_i\), implying the variance is zero if we chose the inclusion probabilities to be proportional to the true values. Of course, we don’t know the true values, so this is hard to do. For DR, \(z_i=y_i-\hy_i\), so the variance will be zero if the signed residual between the true value and the model’s prediction is proportional the inclusion probability.</p>

<p>In order to say anything about which one is better, we’d need to know more about the model estimates \(\hy_i\). Beyond that, comparing the estimators term-by-term is difficult because of the pesky joint inclusion probabilities \(\pi_{i,j}\). For an unspecified sampling design, we can’t say whether the terms \(\pi_i\pi_j-\pi_{i,j}\) are positive or negative, so proving general theorems about the estimators is difficult.</p>

<p>On the other hand, the variance of the model based estimator depends on the model errors and the true rewards</p>

\[\Var(\hmum) = \frac{1}{N^2}\sum_{i,j}\Cov(\Delta(x_i)+y_i,\Delta(x_j)+y_j),\]

<p>which can be smaller than the either of the variances above for a well-calibrated model. In general then, there is a bias-variance tradeoff  in the choices of estimators.</p>


  <small>
    <a href="/research_notes/">Back to all notes</a>
  </small>
</article>
 <!-- "> -->
      </main>

      <!-- Footer  -->
      <div class="container" id="footer">
        <hr>
        <div class='container links'>
          <a href="https://github.com/bchugg" rel='nofollow'><img src="/assets/images/github.png"></a>
          <a href="https://www.linkedin.com/in/ben-chugg-3a4616aa/" rel='nofollow'><img src="/assets/images/linkedin.png"></a>
          <a href="https://www.goodreads.com/user/show/90945992-ben-chugg" rel='nofollow'><img src="/assets/images/goodreads.png"></a>
          <a href="https://twitter.com/BennyChugg" rel='nofollow'><img src="/assets/images/twitter.png"></a>
        </div>
      
        <div class='copyright'>
          <small>&#169; 2022 Ben Chugg</small>
        </div>
      
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>

    // Highlight search Query
    var url = window.location.href;
      if (url.lastIndexOf("?q=") > 0) {
        // get the index of the parameter, add three (to account for length)
        var stringloc = url.lastIndexOf("?q=") + 3;
        // get the substring (query) and decode
        var searchquery = decodeURIComponent(url.substr(stringloc));
        // regex matches at beginning of line, end of line or word boundary, useful for poetry
        var regex = new RegExp("(?:^|\\b)(" + searchquery + ")(?:$|\\b)", "gim");
        // get, add mark and then set content
        var content = document.getElementById("main").innerHTML;
        document.getElementById("main").innerHTML = content.replace(regex, "<mark>$1</mark>");
      }

      // Toggle sidebar
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             !sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

<!-- Facebook SDK for JavaScript -->
<script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '589495744558280',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/en_US/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>
  </body>
</html>
