<!DOCTYPE html>
<html lang="en-us">

  <head prefix="og: http://ogp.me/ns#; dc: http://purl.org/dc/terms/#">
  
  
  <!-- Canonical link to help search engines -->
  <link rel="canonical" href="http://localhost:4000/research_notes/bayesian-post-hoc/" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PN2H9928PZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PN2H9928PZ');
  </script>

  <!-- Basic meta elements -->
  <meta charset="utf-8" />

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width,initial-scale=1.0,shrink-to-fit=no" />

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <title>
    
     Bayesian decision theory is post-hoc valid
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="Bayesian decision theory is post-hoc valid" />
<meta name="author" content="Ben Chugg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="On a very nice feature of Bayesianism that’s missing from frequentist testing" />
<meta property="og:description" content="On a very nice feature of Bayesianism that’s missing from frequentist testing" />
<link rel="canonical" href="http://localhost:4000/research_notes/bayesian-post-hoc/" />
<meta property="og:url" content="http://localhost:4000/research_notes/bayesian-post-hoc/" />
<meta property="og:site_name" content="benny" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-22T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bayesian decision theory is post-hoc valid" />
<meta name="twitter:site" content="@bennychugg" />
<meta name="twitter:creator" content="@Ben Chugg" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Chugg","url":"https://benchugg.com"},"dateModified":"2025-08-22T00:00:00-04:00","datePublished":"2025-08-22T00:00:00-04:00","description":"On a very nice feature of Bayesianism that’s missing from frequentist testing","headline":"Bayesian decision theory is post-hoc valid","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research_notes/bayesian-post-hoc/"},"url":"http://localhost:4000/research_notes/bayesian-post-hoc/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Dublin Core metadata for Zotero -->
  <meta property="dc:title" content="Bayesian decision theory is post-hoc valid" />
  <meta property="dc:creator" content="Ben Chugg" />
  <meta property="dc:identifier" content="http://localhost:4000/research_notes/bayesian-post-hoc/" />
  
  <meta property="dc:date" content="2025-08-22 00:00:00 -0400" />
  
  <meta property="dc:source" content="benny" />

  <!-- Open Graph and Twitter metadata -->
  <meta property="og:title" content="Bayesian decision theory is post-hoc valid" />
  <meta property="og:url" content="http://localhost:4000/research_notes/bayesian-post-hoc/" />
  
    <meta property="og:image" content="http://localhost:4000/assets/images/heads.jpeg"/>
    <meta name="twitter:image" content="http://localhost:4000/assets/images/heads.jpeg" />
  
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:domain" value="benchugg.com" />
  <meta name="twitter:title" value="Bayesian decision theory is post-hoc valid" />
  <meta name="twitter:url" value="http://localhost:4000" />
  
  <!-- Description is dependent on page type  -->
  
    <meta property="og:description" content="On a very nice feature of Bayesianism that's missing from frequentist testing">
    <meta name="twitter:description" value="On a very nice feature of Bayesianism that's missing from frequentist testing" />
    <meta property="og:type" content="article" />
  
  

  <!-- CSS link -->
  <link rel="stylesheet" href="/assets/css/style.css" />

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="167x167" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="180x180" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon.ico">
  <link rel="shortcut icon" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="114×114" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="72×72" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” href="/assets/favicon.ico" />
  <link rel=”icon” href="/assets/favicon.ico", sizes="32x32"/>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml" />

  
</head>


  <body class="theme-base-">


    <!--
      Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS
      collisions with our real content.
    -->
    <div class="wrap">
      <header class="masthead">
        <div class="container">
          <a id='name' href="/">Ben Chugg</a>
          <div class="span">
            <a href="/papers/">Papers</a> 
            <a href="/writing/">Writing</a>
            <a href="/research_notes/">Notes</a>
          </div>
        </div>
      </header>

      <main class="container content" id="main">
        <article class="note">
  <!-- <small>
    <a id="back", href="/research_notes/">Back to all notes</a>
  </small> -->
  <!-- <hr> -->
  
  <p class='title'>Bayesian decision theory is post-hoc valid</p> 
  <p id="date">August 22, 2025</p>
  <br/>
  \[\newcommand{\calA}{\mathcal{A}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Re}{\mathbb{R}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\d}{\text{d}}\]

<p>A common criticism of standard, frequentist Neyman-Pearson hypothesis testing is that it’s extremely brittle under data-dependent choices of the parameters. In particular, the significance level \(\alpha\) must be chosen in advance of seeing the data (<a href="/research_notes/p_values/">we previously studied this in the specific case of p-values</a>). As soon as \(\alpha\) is data-dependent whatsoever, all the guarantees provided by Neyman-Pearson theory begin to break.</p>

<p>One response to this problem is to yell at practitioners until they get it in their heads that they can’t make these illegal post-hoc decisions. We’ve been doing this for many years, with limited success. There’s just too much incentive to p-hack.</p>

<p>Another response is to try and save Neyman-Pearson theory by making it post-hoc valid. This was the goal of <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2205.00901">Peter Grünwald’s 2024 PNAS paper</a>. I’m very sympathetic to this approach, and also wrote <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2508.00770">a paper</a> about it. But there are drawbacks, the biggest being that the kind of guarantee you obtain is different than the standard error probabilities. It’s a heavy lift to convince the entire world to stop reporting p-values. Every social scientist would lose their mind.</p>

<p>A third response is to give up on Neyman-Pearson theory and become a Bayesian. Enough of these type-I error guarantees and asymmetries between the null and the alternative. We’ll just put a prior over everything and update our posteriors, which tells us all we need to know.</p>

<p>This is controversial. I think it’s fair to say that Bayesian methods are in the minority when it comes to applied statistics. Most analysts rely on frequentist tools like p-values, hypothesis tests, and confidence intervals.</p>

<p>But Bayesianism does have a nice property: unlike Neyman-Pearson theory, it’s post-hoc valid! That is, the loss function (their rough equivalent of the significance level) can change as a function of the data. This means that you can gather data, analyze it, change your goals and even what question you’re asking, and then re-analyze. Regardless of how you feel about Bayesian statistics, you have to admit that this is extremely compelling.</p>

<p>But as far as I can tell, that Bayesian theory is post-hoc valid is always stated as folklore. I’ve never seen a formal statement or proof anywhere. So that’s what we’re doing here.</p>

<p>Let \(\Theta\) be a parameter space, \(\calA\) a set of actions, and \(L:\Theta\times \calA\to\Re_{\geq 0}\) a loss function. Associated to each \(\theta\in\Theta\) is a distribution \(P_\theta\). 
Let \(\pi\) be a prior on \(\Theta\). Given observations \(X\), Bayesian decision theory tells us to minimize the expected posterior loss. That is, we take action 
\begin{equation}
\label{eq:bayes_estimator}
    \delta_\pi(X) \equiv \argmin_{a\in\calA} \E_{\theta \sim \pi(\cdot|X)} L(\theta, a). \tag{1}
\end{equation}
A fundamental result is that the <em>Bayes optimal decision rule</em>, henceforth “Bayes decision” for brevity, is always given by \(\delta_\pi\), meaning that it satisfies \(B_\pi(\delta_\pi) = \inf_\delta B_\pi(\delta)\) where 
\begin{equation}
\label{eq:bayes_risk}
    B_\pi(\delta) = \E_{\theta\sim \pi} \E_{X\sim P_\theta}[L(\theta, \delta(X))].  \tag{2}
\end{equation}
(Note that \(\theta\) is drawn from the prior \(\pi\) in \eqref{eq:bayes_risk}, not the posterior \(\pi(\cdot|X)\) as in \eqref{eq:bayes_estimator}.) Bayes estimators are, as the name suggests, the Bayesian equivalent to frequentist minimax estimators, and a unified solution for the Bayes decision regardless of the loss function is a big benefit of Bayesian decision theory.</p>

<p>To move to a post-hoc setting, let’s consider a set of losses \(\{L_b\}_{b\in\calB}\). 
Suppose an adversary is allowed to choose which loss we’re using, and this choice can be data-dependent. That is, the adversary sees the data, then chooses the loss. 
We can encode the adversary as a measurable map \(B:\calX\to\calB\) (where the data \(X\) is taking values in \(\calX\)).</p>

<p>With this setup, Bayesian theory can accommodate a post-hoc selection of the loss in the following sense: If we minimize the expected posterior loss as before but on loss \(L_{B(X)}\) (which we observe), then the resulting decision rule minimizes Bayes risk, where the Bayes risk is now defined in terms of the loss selected by the adversary. More formally:</p>

<p><strong>Proposition.</strong> The decision rule 
\begin{equation}
\phi_\pi(X) = \argmin_{a\in\calA} \E_{\theta\sim \pi(\cdot|X)} L_{B(X)} (\theta, a),
\end{equation}
satisfies 
\begin{equation}
    \E_{\theta\sim\pi} \E_{X\sim P_\theta} L_{B(X)}(\theta, \phi_\pi(X)) = \inf_{\phi} \E_{\theta\sim\pi} \E_{X\sim P_\theta} L_{B(X)}(\theta, \phi(X)), 
\end{equation}
for all \(B:\calX\to\calB\).</p>

<p>Note that \(\phi_\pi\) is well-defined since \(\phi\) is allowed to see \(B(X)\) (if \(\phi\) is not allowed to see \(B(X)\) before making a decision, then the problem is basically impossible to solve. You’d have to assume a worse case loss).</p>

<p>Again, this result isn’t new per se, and the proof isn’t difficult. But I can’t find a similar statement anywhere.</p>

<p>The proof is pretty straightforward. For any decision rule \(\phi\), let \(\ell(\phi(x) \vert x) = \E_{\theta\sim \pi(\cdot\vert x)} L_{B(x)}(\theta, \phi(x))\) be its expected posterior loss at a given \(x\) on loss \(L_{B(x)}\).  For simplicity, assume that \(P_\theta\) has density \(p(\cdot\vert\theta)\). Let \(p(x) = \int p(x\vert\theta)\pi(\theta)\d\theta\) be the marginal and note that \(p(x\vert \theta)\pi(\theta) = \pi(\theta\vert x)p(x)\). Then, by Fubini’s theorem,</p>

\[\begin{align}
    \E_{\theta\sim\pi} \E_{X\sim P_\theta} L_{B(X)}(\theta, \phi(X)) &amp;= \int_\Theta \int_\calX L_{B(x)} (\theta, \phi(x)) p(x|\theta)\pi(\theta)\d x\d\theta \\ 
    &amp;= \int_\calX \int_\Theta L_{B(x)} (\theta, \phi(x)) \pi(\theta|x)p(x)\d \theta\d x \\ 
    &amp;= \int_\calX \ell(\phi(x)|x) p(x) \d\theta.
\end{align}\]

<p>By minimizing the expected posterior \(\ell(\phi(x)\vert x)\) we’re minimizing the above integrand at every \(x\), thus minimizing the value of the integral.</p>

<p>And there we have it. In the recent <a rel="nofollow noopener noreferrer" href="https://arxiv.org/pdf/2508.00770">paper</a> that I mentioned, we also discuss what a notion of type-I error might  look like for Bayesians, and provide a decision rule which satisfies it. If you made it this far, you might like that.</p>


  <small>
    <a link="back", href="/research_notes/">Back to all notes</a>
  </small>
</article>
 <!-- "> -->
      </main>

      <!-- Footer  -->
      <div class="container" id="footer">
        <hr>
        <div class='container links'>
          <a href="https://github.com/bchugg" rel='nofollow'><img src="/assets/images/github.png"></a>
          <a href="https://www.goodreads.com/user/show/90945992-ben-chugg" rel='nofollow'><img src="/assets/images/goodreads.png"></a>
          <a href="https://twitter.com/BennyChugg" rel='nofollow'><img src="/assets/images/twitter.png"></a>
          <a href="https://www.stepstophaeacia.com/" rel='nofollow'><img src="/assets/images/substack.jpeg"></a>
        </div>
      
        <div class='copyright'>
          <small>&#169; 2025 Ben Chugg</small>
        </div>
      
      </div>
    </div>

  </body>
</html> 
