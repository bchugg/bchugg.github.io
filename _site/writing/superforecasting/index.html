<!DOCTYPE html>
<html lang="en-us">

  <head prefix="og: http://ogp.me/ns#; dc: http://purl.org/dc/terms/#">
  
  
  <!-- Canonical link to help search engines -->
  <link rel="canonical" href="http://localhost:4000/writing/superforecasting/" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PN2H9928PZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PN2H9928PZ');
  </script>

  <!-- Basic meta elements -->
  <meta charset="utf-8" />

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width,initial-scale=1.0,shrink-to-fit=no" />

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <title>
    
     An epistemological Frankenstein
    
  </title>

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="An epistemological Frankenstein" />
<meta name="author" content="Ben Chugg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Or: Does anyone take superforcasting seriously?" />
<meta property="og:description" content="Or: Does anyone take superforcasting seriously?" />
<link rel="canonical" href="http://localhost:4000/writing/superforecasting/" />
<meta property="og:url" content="http://localhost:4000/writing/superforecasting/" />
<meta property="og:site_name" content="benny" />
<meta property="og:image" content="http://localhost:4000/assets/writing_images/glassball.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-09-22T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:image" content="http://localhost:4000/assets/writing_images/glassball.jpg" />
<meta property="twitter:title" content="An epistemological Frankenstein" />
<meta name="twitter:site" content="@bennychugg" />
<meta name="twitter:creator" content="@Ben Chugg" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Chugg","url":"https://benchugg.com"},"dateModified":"2023-09-22T00:00:00-04:00","datePublished":"2023-09-22T00:00:00-04:00","description":"Or: Does anyone take superforcasting seriously?","headline":"An epistemological Frankenstein","image":"http://localhost:4000/assets/writing_images/glassball.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/writing/superforecasting/"},"url":"http://localhost:4000/writing/superforecasting/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Dublin Core metadata for Zotero -->
  <meta property="dc:title" content="An epistemological Frankenstein" />
  <meta property="dc:creator" content="Ben Chugg" />
  <meta property="dc:identifier" content="http://localhost:4000/writing/superforecasting/" />
  
  <meta property="dc:date" content="2023-09-22 00:00:00 -0400" />
  
  <meta property="dc:source" content="benny" />

  <!-- Open Graph and Twitter metadata -->
  <meta property="og:title" content="An epistemological Frankenstein" />
  <meta property="og:url" content="http://localhost:4000/writing/superforecasting/" />
  
    <meta property="og:image" content="http://localhost:4000/assets/writing_images/glassball.jpg" />
    <meta name="twitter:image" content="http://localhost:4000/assets/writing_images/glassball.jpg" />
  
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:card" content="summary">
  <meta name="twitter:domain" value="benchugg.com" />
  <meta name="twitter:title" value="An epistemological Frankenstein" />
  <meta name="twitter:url" value="http://localhost:4000" />
  
  <!-- Description is dependent on page type  -->
  
    <meta property="og:description" content="Or: Does anyone take superforcasting seriously?">
    <meta name="twitter:description" value="Or: Does anyone take superforcasting seriously?" />
    <meta property="og:type" content="article" />
  
  

  <!-- CSS link -->
  <link rel="stylesheet" href="/assets/css/style.css" />

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="167x167" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="180x180" href="/assets/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon.ico">
  <link rel="shortcut icon" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="114×114" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” sizes="72×72" href="/assets/favicon.ico" />
  <link rel=”apple-touch-icon” href="/assets/favicon.ico" />
  <link rel=”icon” href="/assets/favicon.ico", sizes="32x32"/>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml" />

  
</head>


  <body class="theme-base-">


    <!--
      Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS
      collisions with our real content.
    -->
    <div class="wrap">
      <header class="masthead">
        <div class="container">
          <a id='name' href="/">Ben Chugg</a>
          <div class="span">
            <a href="/papers/">Papers</a> 
            <a href="/writing/">Writing</a>
            <a href="/research_notes/">Notes</a>
          </div>
        </div>
      </header>

      <main class="container content" id="main">
        <article class="writing">
  <!-- <small>
    <a id="back", href="/writing/">Back to all writing</a>
  </small>
  <hr> -->
  
  <p class='title'>An epistemological Frankenstein</p> 
  <p id="date">September 22, 2023; Updated November 25, 2023</p>
  <br/><br/>
  <p>Superforecasters predict the future. Or try to, at least.</p>

<p>The “science” of superforecasting emerged from Philip Tetlock’s work in the 1990s. He demonstrated that most political pundits were extremely bad at predicting the future—often no better than chance. But a small group of people were consistently better than average, and Tetlock dubbed these enlightened souls superforecasters. Since then, a variety of organizations have been founded to promote and study superforecasting. Chief among them are the <a rel="nofollow noopener noreferrer" href="https://goodjudgment.com/">The Good Judgment Project</a> (GJP), founded by Tetlock himself, and the <a rel="nofollow noopener noreferrer" href="https://forecastingresearch.org/">Forecasting Research Institute</a> (FRI).</p>

<p>If you’re like me—if you’re like most people in fact—this sounds … suspicious, to say the least. I think this skepticism is warranted, and we’ll get to some reasons why soon. But there are also some people who believe in the power of superforecasting, as evidenced by the organizations above. Culturally, the people who seem the most excited about superforecasting are often rationalists (of the Bay area variety), and/or effective altruists. Forecasting is a dominant theme on LessWrong, ACX, and the EA forum, and all the various discords and discussion groups associated with these groups (all those of which I’ve been a part, at least).</p>

<p>Recently, the FRI held a tournament where superforecasters were asked to predict the likelihood of various existential risks. Quoting from the <a rel="nofollow noopener noreferrer" href="https://forecastingresearch.org/news/results-from-the-2022-existential-risk-persuasion-tournament">website</a>: “We asked tournament participants to predict the likelihood of global risks related to nuclear weapon use, biorisks, and AI, along with dozens of other related, shorter-run forecasts.” Here are some of the results (you can see all results <a rel="nofollow noopener noreferrer" href="https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64abffe3f024747dd0e38d71/1688993798938/XPT.pdf">here</a>):</p>

<p><img src="/assets/writing_images/superforecasting_table.png" alt="" /></p>

<p>These are the cumulative results of a bunch of superforecasters (88 to be precise) predicting whether humanity will go extinct from various threats. There are a couple of intriguing results.</p>

<p>For one, superforecasters are a relatively optimistic lot compared to their historical counterparts. Traditionally, those who claim to be peering into the future, however dimly, are prophets of death, ruin, and destruction. Think of your <a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Road_to_Survival">Vogts</a>, <a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Paul_R._Ehrlich">Ehrlichs</a>, <a rel="nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Malthusianism">Malthus’</a>, <a rel="nofollow noopener noreferrer" href="https://vaclavsmil.com/wp-content/uploads/docs/smil-article-2006-worldwatch.pdf">Ivanhoes</a>, and <a rel="nofollow noopener noreferrer" href="https://spectrum.ieee.org/peak-oil-specimen-case-apocalypic-thinking">Hubberts</a>. It’s refreshing that, in this study at least, superforecasters are courting the more hopeful side of future.</p>

<p>Second, there is a big gap between the superforecaster estimates regarding AI and those of the rationalists. Superforecasters assign only a 0.38% chance of extinction by AI, while the rationalist estimates tend to be much higher. In <a rel="nofollow noopener noreferrer" href="https://www.amazon.com/s/?ie=UTF8&amp;keywords=the+precipice&amp;tag=googhydr-20&amp;index=stripbooks&amp;hvadid=241582478615&amp;hvpos=&amp;hvnetw=g&amp;hvrand=9770591019384565647&amp;hvpone=&amp;hvptwo=&amp;hvqmt=e&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1025202&amp;hvtargid=kwd-911701772&amp;ref=pd_sl_7789zjv6zw_e&amp;hydadcr=15272_10334736"><em>The Precipice</em></a>, for instance, Toby Ord estimates a one-in-six chance. Yudkowsky <a rel="nofollow noopener noreferrer" href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">seems to think</a> it’s basically 100%. Scott Alexander <a rel="nofollow noopener noreferrer" href="https://www.astralcodexten.com/p/why-i-am-not-as-much-of-a-doomer">thinks</a> it’s around 33%. Max Tegmark <a rel="nofollow noopener noreferrer" href="https://www.sciencetimes.com/articles/44128/20230603/artificial-intelligence-humanity-species-extinction.htm">says</a> 50%. Paul Cristiano <a rel="nofollow noopener noreferrer" href="https://ai-alignment.com/my-views-on-doom-4788b1cd0c72">says</a> 22%. (Of course, all of these estimates fluctuate a lot because <a href="/writing/commentary_is_all_that_matters/">Bayesianism doesn’t really make any sense in this context</a>, but you get the point.)</p>

<p>These discrepancies give us an opportunity to check how seriously rationalists take superforecasting. Remember: superforecasters are supposed to be far better than any of us at predicting the future. If you are convinced by superforecasting, then a bunch of superforecasters claiming that existential risk from AI is far lower than you previously thought should have a huge effect on your beliefs.</p>

<p>So—what has happened? Have a bunch of prominent AI doomers (who tend to be rationalists) changed their mind all of a sudden, shifting their estimates to be aligned with those of the superforecasters? Not as far as I can tell. (But if you have any examples I’d love to hear from you.) If anything, people are <em>more</em> worried now and <a rel="nofollow noopener noreferrer" href="https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/">calling</a> for all sorts of national and international regulation. Rationalist twitter is like walking into leftist twitter and replacing the word “capitalism” with “AI.”</p>

<p>What’s going on here? The FRI study is incredibly good news for the rationalists, after all. You don’t have to be nearly as worried about AI-risk anymore! There should be a proliferation of awkward <a rel="nofollow noopener noreferrer" href="https://www.astralcodexten.com/p/every-bay-area-house-party">Bay Area parties</a> where everyone is talking about how relieved they are. Now they can focus all their efforts on <a rel="nofollow noopener noreferrer" href="https://www.shrimpwelfareproject.org/">creating shrimp utopias</a> or trying to come up with the <a rel="nofollow noopener noreferrer" href="https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized">one decision theory to rule them all</a>.</p>

<p>The biggest shift in beliefs I’ve seen came from Scott Alexander, who changed his number from 33% to 20-25%. He wrote a <a rel="nofollow noopener noreferrer" href="https://www.astralcodexten.com/p/the-extinction-tournament">thoughtful, self-aware piece</a> pointing out the problem, and then basically throws up his hands and admits it’s hard to justify his refusal to update more. As he says: “it’s the lowest I can make the mysterious-number-generating lobe of my brain go before it threatens to go on strike in protest.”
Scott strikes me as one of the most open-minded, truth-oriented intellectuals out there<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. If he can’t bring himself to take the superforecasters seriously enough to substantially change his estimates, it’s no surprise that no one else can.</p>

<p>Oddly, to anyone <em>but</em> a rationalist, the response of the rationalists is entirely reasonable. The superforecasters are offering arguments. The rationalists think they have better arguments. So of course they won’t change their mind. This is totally normal and is how debate works in the real world. But because rationalists assign so much credibility to superforecasting, disagreeing with them by such huge margins requires some sort of emergency epistemological taskforce. When you believe that some people have a superhuman ability to forecast the future, how on earth can you disagree with them in good conscience?</p>

<p>We’ve witnessed an epistemological Frankenstein: the rationalists invented a class of demi-Gods, the demi-Gods disagree with them about AI, and now the rationalists have left town, hoping the monster they created doesn’t follow. (There’s something deeply ironic about all of this, given that losing control over your own creation is quite literally the core concern of AI-doomers.)</p>

<h1 id="does-superforecasting-work">Does superforecasting work?</h1>

<p>So here’s a question: Do superforecasters actually display an exceptional ability to predict the future? The evidence, when examined by third parties, is surprisingly weak.</p>

<p>In an <a rel="nofollow noopener noreferrer" href="https://progress.institute/can-policymakers-trust-forecasters/">article</a> for The Institute for Progress, Gavin Leech and Misha Yagudin describe how they lost confidence in superforecasting despite being initially optimistic: “[O]n reviewing the available evidence, our confidence in the ‘forecaster advantage’ took a hit.” As one example, they discuss an <a rel="nofollow noopener noreferrer" href="https://www.washingtonpost.com/opinions/david-ignatius-more-chatter-than-needed/2013/11/01/1194a984-425a-11e3-a624-41d661b0bb78_story.html">oft cited statistic</a> that superforecasters perform about 30% better than domain experts. But digging into the data, Leech and Yagudin find that</p>

<blockquote>
  <p>The playing field is actually a bit more level than these eye-catching stats imply. A closer look at the 30% claim led us to an unpublished study that indeed describes a 35% difference – but the paper’s comparison between experts and superforecasters was not apples-to-apples: the aggregation method used for the forecasters is known to produce better results than the one used for the expert analysts. <strong>A fair comparison found a 10% advantage for the amateurs, not reaching statistical significance.</strong> (emphasis added)</p>
</blockquote>

<p>Then there are concrete examples of significant world events not being assigned a high probability by superforecasters. As noted by Leech and Yagudin, forecasters assigned only a 15% chance of Russian invading Ukraine. And throughout February 2020, superforecasters <a rel="nofollow noopener noreferrer" href="https://www.eurasiagroup.net/live-post/superforecaster-fridays-total-cases-coronavirus-reported-who-20-march-2020">put the probability</a> that there would be more than 200,000 Covid-19 cases by March 20, 2020 at around 5%. In fact, there were more than <a rel="nofollow noopener noreferrer" href="https://www.spglobal.com/en/research-insights/articles/covid-19-daily-update-march-20-2020">258,000 <em>confirmed</em> cases</a>. Of course, this doesn’t mean that the superforecasters were <em>wrong</em>, since low probability events do in fact occur. But clearly they were caught off guard. Many other issues, including problems with the evaluation methodology, are nicely summarized by Ben Recht <a rel="nofollow noopener noreferrer" href="https://argmin.substack.com/p/superforecasters-are-not-superheroes">here</a>.</p>

<p>Statistics aside, do people with skin in the game use superforecasting? After all, if superforecasting was reliable, then it would be hugely valuable to companies and governments. Do they use it?</p>

<p>It doesn’t seem like it. Some companies have trialed internal “prediction markets”, but a <a rel="nofollow noopener noreferrer" href="https://forum.effectivealtruism.org/posts/dQhjwHA7LhfE8YpYF/prediction-markets-in-the-corporate-setting">big report</a> found that they failed to gain traction. Of course, it’s possible this is for complex sociological reasons unrelated to the efficacy of these markets, but you’ll forgive me for thinking that if a company could reliably get accurate information about the future they would figure out a way to make it work—Hansonian arguments aside. Meanwhile, the GJP website lists some <a rel="nofollow noopener noreferrer" href="https://goodjudgment.com/resources/case-studies/">case-studies</a> that, presumably, are supposed to illustrate the benefits of superforecasting. Most, however, don’t demonstrate any concrete wins, instead simply listing forecasts that have been made or workshops that GJP has run.</p>

<p>One study, however, does demonstrate a success for superforecasting: a <a rel="nofollow noopener noreferrer" href="https://goodjudgment.com/wp-content/uploads/2023/06/Superforecasting-the-Fed-Target-Rate_6_14_23.pdf">recent whitepaper</a> about forecasting the Fed’s target range. Superforecasters were 66% more accurate (according to a Brier score) than the “CME FedWatch tool.” I have no idea what the FedWatch tool is, but it seems dishonest to not acknowledge that superforecasting worked in this case. So let’s mark this as a win on the ledger. One win among several failures still leaves me somewhere between skeptical and dubious.</p>

<p>In fact, superforecasters <em>themselves</em> seem to be skeptical of the applicability of superforecasting. <a rel="nofollow noopener noreferrer" href="https://www.samstack.io/p/five-questions-for-michael-story">Here is</a> Michael Story acknowledging that superforecasting isn’t “immediately valuable” to people:</p>

<blockquote>
  <p>Forecasting might be overrated. Nearly all forecasters are paid more by their day jobs to do something other than forecasting. The market message is “don’t forecast”! Forecasting websites also don’t exactly get a huge amount of traffic, so it isn’t like huge numbers of people are relying on these forecasts to make important decisions yet. If this was immediately valuable to people, they would be looking at it all the time, and they’re not.</p>
</blockquote>

<p>Alas, for now, it seems like governments still rely on people providing them with reasoned arguments, not numerical probabilities about the future. Here is Story explaining what kind of intel the UK government wanted in the early days of Covid:</p>

<blockquote>
  <p>During the pandemic, Dominic Cummings said some of the most useful stuff that he received and circulated in the British government was not forecasting. It was qualitative information explaining the general model of what’s going on, which enabled decision-makers to think more clearly about their options for action and the likely consequences. If you’re worried about a new disease outbreak, you don’t just want a percentage probability estimate about future case numbers, you want an explanation of how the virus is likely to spread, what you can do about it, how you can prevent it.</p>
</blockquote>

<p>So what do we conclude about superforecasting? Certainly it’s not a magic bullet. If it were, we’d see a priestly class of very rich people in control of everything. But there might be some signal in the noise, however faint. This is not overly surprising. After all, much of superforecasting appears to be adhering to basic principles such as open-mindedness, changing your worldview as new information becomes available, and being open to criticism. As Tetlock puts it, superforecasters aren’t actually using Bayes’ theorem or fancy math. They’re simply willing to change their minds upon receiving new information and arguments:</p>

<blockquote>
  <p>The superforecasters are a numerate bunch: many know about Bayes’ theorem and could deploy it if they felt it was worth the trouble. But they rarely crunch the numbers so explicitly. What matters far more to the superforecasters than Bayes’ theorem is Bayes’ core insight of gradually getting closer to the truth by constantly updating in proportion to the weight of the evidence.
<em>Superforecasting</em>, pg 238</p>
</blockquote>

<p>It’s unsurprising that someone with epistemic humility outperforms an ideologue wedded to a single perspective. This doesn’t give you magical abilities to peer into the future, but it can make you see reality more clearly. Believing that superforecasters <em>have</em> unlocked the ability to pull back the veil on the future puts those that disagree with them at an impass.  Unless, of course,  superforecasting only works when you agree with the results?</p>

<p><em>Thanks to <a rel="nofollow noopener noreferrer" href="https://falliblepieces.substack.com/">Cam</a> for comments.</em></p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>I’m basing this assessment on that weird, utterly asymmetric parasocial relationship that you have with your favorite writers and podcasters. The one where you feel you know them quite well but they haven’t spent even one second in their entire lives thinking about you. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
    </li>
  </ol>
</div>

  <small>
    <a id="back", href="/writing/">Back to all writing</a>
  </small>
  <br><br>
  <p id="subscribe"><a href="/subscribe/">Subscribe</a> to get notified about new essays.</p>
</article>
 <!-- "> -->
      </main>

      <!-- Footer  -->
      <div class="container" id="footer">
        <hr>
        <div class='container links'>
          <a href="https://github.com/bchugg" rel='nofollow'><img src="/assets/images/github.png"></a>
          <a href="https://www.goodreads.com/user/show/90945992-ben-chugg" rel='nofollow'><img src="/assets/images/goodreads.png"></a>
          <a href="https://twitter.com/BennyChugg" rel='nofollow'><img src="/assets/images/twitter.png"></a>
        </div>
      
        <div class='copyright'>
          <small>&#169; 2025 Ben Chugg</small>
        </div>
      
      </div>
    </div>

  </body>
</html> 
